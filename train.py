# ========== 0. å¯¼å…¥æ‰€éœ€åº“ ==========
import torch
import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback # å¯¼å…¥æ—©åœå›è°ƒ
)
from peft import LoraConfig, TaskType, get_peft_model
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========== 1. åŠ è½½æ‚¨å¤„ç†å¥½çš„æ³•å¾‹æ•°æ®é›† ==========
# âœ… ä½¿ç”¨æ–°çš„æ•°æ®é›†
DATA_FILE = "dataset_final_moreversion.jsonl"
print(f"--- æ­¥éª¤ 1: æ­£åœ¨åŠ è½½ {DATA_FILE} æ–‡ä»¶ ---")
dataset = load_dataset('json', data_files=DATA_FILE, split='train')

# âœ… æ–°å¢ï¼šåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
dataset = dataset.train_test_split(test_size=0.05, seed=42) # 95% è®­ç»ƒ, 5% éªŒè¯
train_dataset = dataset["train"]
eval_dataset = dataset["test"]
print("æ•°æ®é›†åŠ è½½å¹¶åˆ’åˆ†æˆåŠŸï¼")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_dataset)}")


# ========== 2. åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹ ==========
model_name_or_path = '/root/autodl-tmp/legal_finetune/deepseek' 
print(f"--- æ­¥éª¤ 2: æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åˆå§‹åŒ–: {model_name_or_path} ---")

tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    use_fast=False,
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    # å¤§éƒ¨åˆ†æ—¶å€™ï¼Œ`eos_token` ä½œä¸º `pad_token` æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©
    tokenizer.pad_token = tokenizer.eos_token

# ========== 3. æ•°æ®é¢„å¤„ç†å‡½æ•° ==========
def process_func(example):
    MAX_LENGTH = 1024 # æ ¹æ®æ‚¨çš„æ¨¡å‹å’Œæ•°æ®è°ƒæ•´
    tokenized = tokenizer(
        example['text'],
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized

print("--- æ­¥éª¤ 3: æ­£åœ¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯å’Œæ ‡ç­¾åŒ–å¤„ç† ---")
tokenized_train_dataset = train_dataset.map(process_func, remove_columns=train_dataset.column_names)
tokenized_eval_dataset = eval_dataset.map(process_func, remove_columns=eval_dataset.column_names)
print("æ•°æ®å¤„ç†å®Œæˆï¼")


# ========== 4. åŠ è½½æ¨¡å‹ ==========
print(f"--- æ­¥éª¤ 4: æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹... ---")
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype="auto",
    trust_remote_code=True,
    device_map="auto"
)
model.enable_input_require_grads()


# ========== 5. LoRA é…ç½® ==========
print("--- æ­¥éª¤ 5: æ­£åœ¨é…ç½®LoRA ---")
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)
model = get_peft_model(model, config)
model.print_trainable_parameters()


# ========== 6. è®­ç»ƒå‚æ•° ==========
print("--- æ­¥éª¤ 6: æ­£åœ¨è®¾ç½®è®­ç»ƒå‚æ•° ---")
OUTPUT_DIR = "./output_deepseek_legal_lora_v2"
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4, # æ ¹æ®æ‚¨çš„æ˜¾å­˜å¤§å°è°ƒæ•´
    gradient_accumulation_steps=8, # å®é™… batch_size = 4 * 8 = 32
    logging_steps=10,
    num_train_epochs=5, # è®¾ç½®ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„epochæ•°ï¼Œè®©æ—©åœæœºåˆ¶è‡ªåŠ¨å†³å®šä½•æ—¶åœæ­¢
    save_strategy="steps",
    save_steps=50, # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    learning_rate=2e-5, # ä¸ºå¾®è°ƒä»»åŠ¡é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„å­¦ä¹ ç‡
    save_on_each_node=True,
    gradient_checkpointing=True,
    save_safetensors=True,

    # âœ… æ–°å¢ï¼šå¯ç”¨è¯„ä¼°å’Œæ—©åœ
    evaluation_strategy="steps",          # æ¯Næ­¥åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ä¸€æ¬¡
    eval_steps=50,                        # ä¸save_stepsä¿æŒä¸€è‡´ï¼Œæ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
    load_best_model_at_end=True,          # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="loss",         # ä»¥éªŒè¯é›†æŸå¤±ä½œä¸ºæœ€ä½³æ¨¡å‹çš„è¯„åˆ¤æ ‡å‡†
    greater_is_better=False,              # æŸå¤±è¶Šå°è¶Šå¥½
)

# âœ… æ–°å¢ï¼šå®šä¹‰æ—©åœå›è°ƒ
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3, # å¦‚æœéªŒè¯æŸå¤±è¿ç»­3æ¬¡è¯„ä¼°éƒ½æ²¡æœ‰æ”¹å–„ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    early_stopping_threshold=0.01, # æ”¹å–„å¿…é¡»è¶…è¿‡è¿™ä¸ªé˜ˆå€¼æ‰ç®—æ•°
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset, # âœ… ä¼ å…¥éªŒè¯é›†
    data_collator=data_collator,
    callbacks=[early_stopping_callback], # âœ… åº”ç”¨æ—©åœå›è°ƒ
)

# ========== 7. å¯åŠ¨è®­ç»ƒ ==========
print("--- æ­¥éª¤ 7: æ‰€æœ‰å‡†å¤‡å°±ç»ªï¼Œå³å°†å¼€å§‹LoRAå¾®è°ƒï¼---")
trainer.train()

# ========== 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ==========
print("--- æ­¥éª¤ 8: è®­ç»ƒç»“æŸï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹ ---")
# ä¿å­˜æœ€ç»ˆçš„LoRAé€‚é…å™¨
final_model_path = f"{OUTPUT_DIR}/final_model"
trainer.save_model(final_model_path)
print(f"\n--- ğŸ‰ å¾®è°ƒè®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜åœ¨: {final_model_path} ---")

# ä¿å­˜è®­ç»ƒå†å²è®°å½•ï¼Œä»¥ä¾¿ç»˜å›¾
with open(f"{OUTPUT_DIR}/training_log_history.json", "w") as f:
    json.dump(trainer.state.log_history, f, indent=4)
