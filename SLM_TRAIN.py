# ========== 0. å¯¼å…¥æ‰€éœ€åº“ ==========
import torch
import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from peft import LoraConfig, TaskType, get_peft_model
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========== 1. åŠ è½½æˆ‘ä»¬æ–°ç”Ÿæˆçš„å¾®è°ƒæ•°æ®é›† ==========
# âœ… ä½¿ç”¨æˆ‘ä»¬æ–°åˆæˆçš„æ•°æ®é›†
DATA_FILE = "synthetic_query_rewriter_dataset_robust_1k.jsonl"
print(f"--- æ­¥éª¤ 1: æ­£åœ¨åŠ è½½ {DATA_FILE} æ–‡ä»¶ ---")
dataset = load_dataset('json', data_files=DATA_FILE, split='train')

# âœ… åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (95% è®­ç»ƒ, 5% éªŒè¯)
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]
print("æ•°æ®é›†åŠ è½½å¹¶åˆ’åˆ†æˆåŠŸï¼")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_dataset)}")


# ========== 2. åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹ ==========
model_name_or_path = '/root/autodl-tmp/legal_finetune/deepseek'
print(f"--- æ­¥éª¤ 2: æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åˆå§‹åŒ–: {model_name_or_path} ---")

tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    use_fast=False,
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ========== 3. æ•°æ®é¢„å¤„ç†å‡½æ•° (æ ¸å¿ƒæ”¹åŠ¨) ==========
# æˆ‘ä»¬éœ€è¦å°† instruction, input, output æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„prompt
def format_and_tokenize(example):
    MAX_LENGTH = 768 # å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œ768çš„é•¿åº¦è¶³å¤Ÿäº†

    # æ„å»ºä¸€ä¸ªæ ‡å‡†çš„æŒ‡ä»¤è·Ÿéšæ ¼å¼
    prompt_template = (
        "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æŸ¥è¯¢åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æ³•å¾‹é—®é¢˜ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONå¯¹è±¡ï¼Œ"
        "åŒ…å«ç”¨äºå…³é”®è¯æœç´¢çš„'keywords_for_search'å’Œç”¨äºå‘é‡æœç´¢çš„'query_for_vector_search'ã€‚\n\n"
        "### ç”¨æˆ·é—®é¢˜:\n{input}\n\n### JSONè¾“å‡º:\n{output}"
    )
    
    text = prompt_template.format(
        input=example['input'],
        output=example['output']
    ) + tokenizer.eos_token # åœ¨æœ«å°¾æ·»åŠ ç»“æŸç¬¦

    tokenized = tokenizer(
        text,
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )
    # åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è®©æ¨¡å‹é¢„æµ‹æ•´ä¸ªåºåˆ—ï¼Œæ‰€ä»¥labelså°±æ˜¯input_idsçš„æ‹·è´
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized

print("--- æ­¥éª¤ 3: æ­£åœ¨å¯¹æ•°æ®é›†è¿›è¡Œæ ¼å¼åŒ–å’Œåˆ†è¯å¤„ç† ---")
tokenized_train_dataset = train_dataset.map(format_and_tokenize, remove_columns=train_dataset.column_names)
tokenized_eval_dataset = eval_dataset.map(format_and_tokenize, remove_columns=eval_dataset.column_names)
print("æ•°æ®å¤„ç†å®Œæˆï¼")


# ========== 4. åŠ è½½åŸºç¡€æ¨¡å‹ ==========
print(f"--- æ­¥éª¤ 4: æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹... ---")
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype="auto",
    trust_remote_code=True,
    device_map="auto"
)
model.enable_input_require_grads()


# ========== 5. LoRA é…ç½® ==========
print("--- æ­¥éª¤ 5: æ­£åœ¨é…ç½®LoRA ---")
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    # é€‚é…DeepSeek-LLM-7Bæ¨¡å‹çš„æ¨¡å—å
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)
model = get_peft_model(model, config)
model.print_trainable_parameters()


# ========== 6. è®­ç»ƒå‚æ•° ==========
print("--- æ­¥éª¤ 6: æ­£åœ¨è®¾ç½®è®­ç»ƒå‚æ•° ---")
# âœ… ä½¿ç”¨æ–°çš„è¾“å‡ºç›®å½•ï¼Œé¿å…è¦†ç›–
OUTPUT_DIR = "./output_query_rewriter_lora"
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8, # å®é™… batch_size = 4 * 8 = 32
    logging_steps=5, # æ›´é¢‘ç¹åœ°è®°å½•æ—¥å¿—
    num_train_epochs=5,
    save_strategy="steps",
    save_steps=15, # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´ï¼Œæ›´é¢‘ç¹åœ°ä¿å­˜å’Œè¯„ä¼°
    learning_rate=2e-5,
    save_on_each_node=True,
    gradient_checkpointing=True,
    save_safetensors=True,
    # âœ… å¯ç”¨è¯„ä¼°å’Œæ—©åœ
    evaluation_strategy="steps",
    eval_steps=15, # ä¸save_stepsä¿æŒä¸€è‡´
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
)

# âœ… å®šä¹‰æ—©åœå›è°ƒ
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=5, # ç¨å¾®å¢åŠ è€å¿ƒï¼Œå› ä¸ºåˆæœŸæŸå¤±å¯èƒ½æ³¢åŠ¨
    early_stopping_threshold=0.005,
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    data_collator=data_collator,
    callbacks=[early_stopping_callback],
)

# ========== 7. å¯åŠ¨è®­ç»ƒ ==========
print("--- æ­¥éª¤ 7: æ‰€æœ‰å‡†å¤‡å°±ç»ªï¼Œå³å°†å¼€å§‹æŸ¥è¯¢é‡å†™æ¨¡å‹çš„LoRAå¾®è°ƒï¼---")
trainer.train()

# ========== 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒæ—¥å¿— ==========
print("--- æ­¥éª¤ 8: è®­ç»ƒç»“æŸï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹ ---")
final_model_path = f"{OUTPUT_DIR}/final_model"
trainer.save_model(final_model_path)
print(f"\n--- ğŸ‰ æŸ¥è¯¢é‡å†™æ¨¡å‹å¾®è°ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜åœ¨: {final_model_path} ---")

# âœ… ä¿å­˜è®­ç»ƒå†å²è®°å½•åˆ°æ–°çš„è¾“å‡ºç›®å½•
log_history_path = f"{OUTPUT_DIR}/training_log_history.json"
with open(log_history_path, "w") as f:
    json.dump(trainer.state.log_history, f, indent=4)
print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³: {log_history_path}")
