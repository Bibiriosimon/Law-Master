import json
import time
import faiss
import numpy as np
import torch
from torch.nn import functional as F
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# ========== 1. é…ç½®å‚æ•° ==========
# --- è¾“å…¥æ–‡ä»¶ ---
# ä½¿ç”¨æˆ‘ä»¬ä¸Šä¸€æ­¥é«˜é€Ÿå¤„ç†åç”Ÿæˆçš„å¢å¼ºç‰ˆè¯­æ–™åº“
ENRICHED_CORPUS_PATH = 'corpus_enriched_fast.jsonl' 

# --- æœ¬åœ°æ¨¡å‹è·¯å¾„ ---
# è¯·ç¡®ä¿è¿™ä¸ªè·¯å¾„æ˜¯æ­£ç¡®çš„
LOCAL_MODEL_PATH = '/root/autodl-tmp/legal_finetune/text2vec-base-chinese' 

# --- è¾“å‡ºæ–‡ä»¶ ---
INDEX_SAVE_PATH = 'law_enhanced_vector_db.faiss'
MAPPING_SAVE_PATH = 'index_to_chunk_map.json' # è¿™ä¸ªæ˜ å°„æ–‡ä»¶å¯¹äºæ£€ç´¢è‡³å…³é‡è¦

# --- è®¡ç®—è®¾å¤‡ä¸æ‰¹å¤„ç†å¤§å° ---
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 64 # å¯ä»¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜é€‚å½“è°ƒæ•´

# ========== 2. è¾…åŠ©å‡½æ•°ä¸æ•°æ®åŠ è½½ ==========

def mean_pooling(model_output, attention_mask):
    """
    å¹³å‡æ± åŒ– - ä»Token Embeddingsè®¡ç®—å¥å­Embeddingçš„æ ‡å‡†æ–¹æ³•ã€‚
    """
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask

def load_enriched_corpus(file_path):
    """
    åŠ è½½å¢å¼ºåçš„æ³•å¾‹è¯­æ–™åº“ (.jsonlæ ¼å¼)ã€‚
    """
    chunks = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))
    return chunks

# ========== 3. ä¸»æ‰§è¡Œæµç¨‹ ==========

if __name__ == "__main__":
    print("--- å‘é‡æ•°æ®åº“æ„å»ºæµç¨‹å¯åŠ¨ ---")

    # --- æ­¥éª¤ 1: åŠ è½½çŸ¥è¯†åº“ ---
    print(f"\n--- æ­¥éª¤ 1: ä» '{ENRICHED_CORPUS_PATH}' åŠ è½½å¢å¼ºåçš„çŸ¥è¯†åº“...")
    try:
        chunks = load_enriched_corpus(ENRICHED_CORPUS_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½äº† {len(chunks)} æ¡æ–‡æ¡£ã€‚")
        if not chunks:
            raise ValueError("é”™è¯¯ï¼šæœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ã€‚")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}"); exit()

    # --- æ­¥éª¤ 2: å‡†å¤‡ç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬ ---
    print("\n--- æ­¥éª¤ 2: å‡†å¤‡ç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬ï¼ˆåˆå¹¶å†…å®¹ä¸é—®é¢˜ï¼‰...")
    texts_to_embed = []
    for chunk in chunks:
        questions_str = "\n".join(chunk.get("hypothetical_questions", []))
        combined_text = f"ç›¸å…³é—®é¢˜ï¼š\n{questions_str}\n\næ³•å¾‹æ¡æ–‡ï¼š\n{chunk['content']}"
        texts_to_embed.append(combined_text)
    print("âœ… æ–‡æœ¬å‡†å¤‡å®Œæˆã€‚ç¤ºä¾‹å¦‚ä¸‹:")
    print("="*25 + "\n" + texts_to_embed[0] + "\n" + "="*25)
    
    # --- æ­¥éª¤ 3: ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"\n--- æ­¥éª¤ 3: ä» '{LOCAL_MODEL_PATH}' åŠ è½½æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)
        model = AutoModel.from_pretrained(LOCAL_MODEL_PATH).to(DEVICE)
        model.eval()
        print(f"âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œå°†è¿è¡Œåœ¨: {DEVICE}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚é”™è¯¯: {e}"); exit()

    # --- æ­¥éª¤ 4: è¿›è¡Œæ–‡æœ¬å‘é‡åŒ– ---
    print(f"\n--- æ­¥éª¤ 4: å¼€å§‹è¿›è¡Œæ–‡æœ¬å‘é‡åŒ– (å…± {len(texts_to_embed)} æ¡)...")
    start_time = time.time()
    all_embeddings = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
    for i in tqdm(range(0, len(texts_to_embed), BATCH_SIZE), desc="å‘é‡åŒ–è¿›åº¦"):
        batch_texts = texts_to_embed[i:i + BATCH_SIZE]
        
        # ä½¿ç”¨æ‚¨çš„åˆ†è¯å’Œç¼–ç é€»è¾‘
        encoded_input = tokenizer(
            batch_texts, 
            padding=True, 
            truncation=True, 
            max_length=512, 
            return_tensors='pt'
        ).to(DEVICE)
        
        with torch.no_grad():
            model_output = model(**encoded_input)
        
        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
        # L2 å½’ä¸€åŒ–
        normalized_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
        
        all_embeddings.append(normalized_embeddings.cpu().numpy())

    embeddings = np.vstack(all_embeddings).astype('float32')
    end_time = time.time()
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’ã€‚")
    print(f"å‘é‡çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")

    # --- æ­¥éª¤ 5: æ„å»ºå¹¶ä¿å­˜FAISSç´¢å¼• ---
    print("\n--- æ­¥éª¤ 5: æ„å»ºå¹¶ä¿å­˜FAISSç´¢å¼•...")
    try:
        d = embeddings.shape[1]
        # ä½¿ç”¨ IndexFlatL2 è¿›è¡Œç²¾ç¡®çš„L2è·ç¦»æœç´¢
        index = faiss.IndexFlatL2(d)
        
        # ä½¿ç”¨ IndexIDMap å°†å‘é‡çš„é¡ºåºç´¢å¼• (0, 1, 2, ...) ä¿å­˜ä¸‹æ¥
        # è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘é‡çš„IDç›´æ¥æ˜ å°„å›åŸå§‹æ•°æ®
        ids = np.arange(len(chunks))
        index = faiss.IndexIDMap(index)
        index.add_with_ids(embeddings, ids)

        faiss.write_index(index, INDEX_SAVE_PATH)
        print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±åŒ…å« {index.ntotal} ä¸ªå‘é‡ã€‚")
        print(f"ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜è‡³: '{INDEX_SAVE_PATH}'")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ„å»ºæˆ–ä¿å­˜FAISSç´¢å¼•å¤±è´¥: {e}")

    # --- æ­¥éª¤ 6: åˆ›å»ºå¹¶ä¿å­˜IDåˆ°æ•°æ®å—çš„æ˜ å°„æ–‡ä»¶ ---
    print(f"\n--- æ­¥éª¤ 6: åˆ›å»ºå¹¶ä¿å­˜ç´¢å¼•IDåˆ°åŸå§‹æ•°æ®çš„æ˜ å°„æ–‡ä»¶...")
    try:
        # è¿™ä¸ªæ˜ å°„å…³ç³»æ˜¯RAGæ£€ç´¢å¬å›åè·å–åŸæ–‡çš„å…³é”®
        index_to_chunk_map = {i: chunk for i, chunk in enumerate(chunks)}
        with open(MAPPING_SAVE_PATH, 'w', encoding='utf-8') as f:
            json.dump(index_to_chunk_map, f, ensure_ascii=False, indent=4)
        print(f"âœ… æ˜ å°„æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: '{MAPPING_SAVE_PATH}'")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åˆ›å»ºæˆ–ä¿å­˜æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")

    print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼å‘é‡æ•°æ®åº“åŠæ˜ å°„æ–‡ä»¶å…¨éƒ¨åˆ›å»ºæˆåŠŸï¼ğŸ‰ğŸ‰ğŸ‰")