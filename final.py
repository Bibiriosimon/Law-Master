import json
import time
import faiss
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from torch.nn import functional as F
# --- 1. å…¨å±€é…ç½® (è¯·æ ¹æ®æ‚¨çš„ç¯å¢ƒä¿®æ”¹è¿™é‡Œçš„è·¯å¾„) ---

# --- æ£€ç´¢å™¨ç»„ä»¶è·¯å¾„ ---
INDEX_PATH = 'final_law_db.index'
KNOWLEDGE_BASE_PATH = 'merged_knowledge_base.json'
EMBEDDING_MODEL_PATH = './text2vec-base-chinese' 

# --- å¤§è¯­è¨€æ¨¡å‹ç»„ä»¶è·¯å¾„ ---
# !!! æ ¸å¿ƒï¼šå·²æ ¹æ®æ‚¨çš„ä¿¡æ¯æ›´æ–°ä¸ºæœ¬åœ°åŸºç¡€æ¨¡å‹è·¯å¾„ !!!
BASE_MODEL_PATH = './deepseek' 
LORA_ADAPTER_PATH = './output_deepseek_legal_lora/checkpoint-1250'

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# --- 2. æ£€ç´¢å™¨ (Retriever) ---
class LawRetriever:
    def __init__(self, index_path, docs_path, embedding_model_path):
        print("å¼€å§‹åŠ è½½æ£€ç´¢å™¨...")
        with open(docs_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        print(f"âœ… çŸ¥è¯†åº“åŸæ–‡åŠ è½½å®Œæˆï¼Œå…± {len(self.documents)} æ¡ã€‚")

        self.index = faiss.read_index(index_path)
        print("âœ… FAISSç´¢å¼•åŠ è½½å®Œæˆã€‚")

        from transformers import AutoTokenizer as EmbeddingTokenizer, AutoModel as EmbeddingModel
        from torch.nn import functional as F

        self.embedding_tokenizer = EmbeddingTokenizer.from_pretrained(embedding_model_path)
        self.embedding_model = EmbeddingModel.from_pretrained(embedding_model_path).to(DEVICE)
        self.embedding_model.eval()
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def retrieve(self, query: str, k: int = 5) -> list[str]:
        with torch.no_grad():
            encoded_input = self.embedding_tokenizer([query], padding=True, truncation=True, max_length=512, return_tensors='pt').to(DEVICE)
            model_output = self.embedding_model(**encoded_input)
            query_embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])
            query_embedding = F.normalize(query_embedding, p=2, dim=1).cpu().numpy()

        distances, indices = self.index.search(query_embedding, k)
        retrieved_docs = [self.documents[i] for i in indices[0]]
        return retrieved_docs

# --- 3. åŠ è½½åº”ç”¨äº†LoRAçš„è¯­è¨€æ¨¡å‹ ---
def load_model_with_lora(base_model_path, lora_path):
    print(f"å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    print(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    print(f"å¼€å§‹åŠ è½½å¹¶åˆå¹¶LoRAé€‚é…å™¨: {lora_path}...")
    model = PeftModel.from_pretrained(model, lora_path)
    print(f"âœ… LoRAé€‚é…å™¨åŠ è½½å¹¶åˆå¹¶å®Œæˆã€‚")

    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    return model, tokenizer

# --- 4. æ ¸å¿ƒæ¨ç†é€»è¾‘ ---
prompt_template_finding = """
# è§’è‰²
ä½ æ˜¯ä¸€åèµ„æ·±çš„ä¸­å›½æ³•å¾‹ä¸“å®¶ï¼Œä»»åŠ¡æ˜¯èµ·è‰ä¸€ä»½ä¸“ä¸šçš„æ³•å¾‹æ¡ˆä»¶åˆæ­¥åˆ†ææŠ¥å‘Šã€‚

# ä»»åŠ¡
ä¸¥æ ¼æ ¹æ®ä¸‹æ–‡æä¾›çš„[ç›¸å…³æ³•æ¡]ï¼Œç»“åˆ[ç”¨æˆ·é—®é¢˜]ï¼Œæ’°å†™ä¸€ä»½ç»“æ„åŒ–ã€é€»è¾‘æ¸…æ™°çš„â€œæ–‡ä¹¦åˆ¤å†³â€ã€‚

# è¦æ±‚
1.  **ä¸¥æ ¼å¾ªè¯**: ä½ çš„æ¯ä¸€é¡¹åˆ†æå’Œç»“è®ºï¼Œéƒ½å¿…é¡»æ˜ç¡®å¼•ç”¨[ç›¸å…³æ³•æ¡]ä¸­çš„å…·ä½“åŸæ–‡ä½œä¸ºä¾æ®ï¼Œä¾‹å¦‚ï¼šâ€œæ ¹æ®ã€Šå·¥ä¼¤ä¿é™©æ¡ä¾‹ã€‹ç¬¬åå››æ¡çš„è§„å®š...â€ã€‚
2.  **ç»“æ„æ¸…æ™°**: è¯·æŒ‰ç…§â€œäº‹å®æ¢³ç†â€ã€â€œæ³•å¾‹é€‚ç”¨åˆ†æâ€å’Œâ€œåˆæ­¥ç»“è®ºâ€ä¸‰ä¸ªéƒ¨åˆ†è¿›è¡Œæ’°å†™ã€‚
3.  **è¯­è¨€ä¸“ä¸š**: ä½¿ç”¨ä¸¥è°¨ã€å®¢è§‚ã€æ­£å¼çš„æ³•å¾‹ä¸“ä¸šæœ¯è¯­ã€‚
4.  **æ ¼å¼è§„èŒƒ**: ä½ çš„è¾“å‡ºåº”æ˜¯å®Œæ•´çš„æ–‡ä¹¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹ã€XMLæ ‡ç­¾ï¼ˆå¦‚</think>ï¼‰æˆ–å…¶ä»–æ— å…³å­—ç¬¦ã€‚

# è¾“å…¥ä¿¡æ¯
[ç›¸å…³æ³•æ¡]
{context}

[ç”¨æˆ·é—®é¢˜]
{query}

# è¾“å‡ºæŠ¥å‘Š
[ä½ çš„åˆ†æå’Œæ–‡ä¹¦åˆ¤å†³]
"""
prompt_template_action = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½å……æ»¡äººæƒ…å‘³ä¸”ç»éªŒä¸°å¯Œçš„æ³•å¾‹æ´åŠ©é¡¾é—®ã€‚ä½ çš„æ²Ÿé€šå¯¹è±¡æ˜¯ä¸€ä½å¯èƒ½æ­£å¤„äºç„¦è™‘å’Œå›°æƒ‘ä¸­çš„æ™®é€šäººã€‚

# ä»»åŠ¡
åŸºäºå·²æœ‰çš„[åˆæ­¥çš„æ–‡ä¹¦åˆ¤å†³]å’Œ[ç›¸å…³æ³•æ¡]ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ä»½æ¸©æš–ã€æ¸…æ™°ã€å……æ»¡é¼“åŠ±çš„è¡ŒåŠ¨æŒ‡å—ã€‚

# è¦æ±‚
1.  **è¯­æ°”å’Œé£æ ¼**: ä½ çš„è¯­æ°”å¿…é¡»æ˜¯**å¹³æ˜“è¿‘äººã€å……æ»¡é¼“åŠ±ä¸”æœ‰åŒç†å¿ƒçš„**ã€‚è¯·åƒå’Œæœ‹å‹èŠå¤©ä¸€æ ·ï¼Œç”¨å¤§ç™½è¯è§£é‡Šå¤æ‚çš„æ³•å¾‹é—®é¢˜ã€‚**è¯·å°†æ‰€æœ‰å»ºè®®æ•´åˆæˆä¸€æ®µæˆ–å‡ æ®µè¿è´¯çš„æ–‡å­—ï¼Œç»å¯¹ä¸è¦ä½¿ç”¨ç”Ÿç¡¬çš„æ•°å­—åˆ—è¡¨ï¼ˆå¦‚1ã€2ã€3...ï¼‰ã€‚**
2.  **å†…å®¹ç»“æ„ (Chain of Thought)**:
    * **é¦–å…ˆï¼Œå®‰æŠšå’Œå…±æƒ…**ï¼šç”¨æ¸©æš–çš„è¯è¯­è‚¯å®šç”¨æˆ·ç»´æƒçš„å‹‡æ°”ï¼Œå¹¶ç”¨æœ€é€šä¿—çš„è¯­è¨€è§£é‡Šæ ¸å¿ƒæ³•æ¡çš„å«ä¹‰ï¼Œè®©ä»–/å¥¹çŸ¥é“â€œæ³•å¾‹æ˜¯ç«™åœ¨ä½ è¿™è¾¹çš„â€ã€‚
    * **å…¶æ¬¡ï¼Œè§£è¯»åˆ¤å†³**ï¼šç®€å•è¯´æ˜ä¸€ä¸‹â€œæ–‡ä¹¦åˆ¤å†³â€çš„ç»“è®ºå¯¹å½“äº‹äººæ„å‘³ç€ä»€ä¹ˆï¼Œç»™äºˆå…¶ä¿¡å¿ƒã€‚
    * **æœ€åï¼Œç»™å‡ºæ¸…æ™°è·¯å¾„**ï¼šå°†ç»´æƒçš„æ­¥éª¤èåˆæˆä¸€ä¸ªæµç•…çš„è¡ŒåŠ¨è·¯çº¿å›¾ã€‚è¦éå¸¸å…·ä½“ï¼Œä¾‹å¦‚ï¼šâ€œç¬¬ä¸€æ­¥ï¼Œä½ éœ€è¦å¸¦ç€è¿™äº›ææ–™ï¼Œå»è¿™ä¸ªåœ°æ–¹...â€ã€â€œä¸ç”¨æ‹…å¿ƒï¼Œæ³•å¾‹è§„å®šäº†ä»–ä»¬å¿…é¡»åœ¨å¤šé•¿æ—¶é—´å†…ç»™ä½ ç­”å¤...â€ã€â€œå¦‚æœé‡åˆ°è¿™ç§æƒ…å†µï¼Œä½ å¯ä»¥æ¥ç€è¿™æ ·åš...â€ã€‚
3.  **ç»“å°¾é¼“åŠ±**: åœ¨æœ€åï¼Œè¯·å†æ¬¡ç»™äºˆç”¨æˆ·åŠ›é‡å’Œæ”¯æŒã€‚

# è¾“å…¥ä¿¡æ¯
[ç›¸å…³æ³•æ¡]
{context}

[ç”¨æˆ·é—®é¢˜]
{query}

[åˆæ­¥çš„æ–‡ä¹¦åˆ¤å†³]
{finding}

# è¾“å‡ºæŒ‡å—
[ä½ çš„æ€è€ƒå’Œè¡ŒåŠ¨æªæ–½]
"""

def generate_response(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    # æ³¨æ„ï¼š1.5Bçš„æ¨¡å‹å¯èƒ½ä¸éœ€è¦ç‰¹åˆ«é•¿çš„max_new_tokens
    outputs = model.generate(**inputs, max_new_tokens=768, do_sample=True, top_p=0.9, temperature=0.6)
    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    return response

def run_rag_chain(query, retriever, llm, llm_tokenizer):
    print("\n" + "="*50)
    print(f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜: {query}")
    print("="*50)

    print("\nğŸ” æ­¥éª¤ä¸€ï¼šæ£€ç´¢ç›¸å…³æ³•æ¡...")
    retrieved_context = retriever.retrieve(query, k=5)
    context_str = "\n\n".join(retrieved_context)
    print("âœ… æ£€ç´¢å®Œæˆã€‚")

    print("\nğŸ§  æ­¥éª¤äºŒï¼šç”Ÿæˆâ€œæ–‡ä¹¦åˆ¤å†³â€...")
    prompt1 = prompt_template_finding.format(context=context_str, query=query)
    preliminary_finding = generate_response(llm, llm_tokenizer, prompt1)
    print("âœ… â€œæ–‡ä¹¦åˆ¤å†³â€å·²ç”Ÿæˆã€‚")

    print("\nğŸš€ æ­¥éª¤ä¸‰ï¼šç”Ÿæˆâ€œè¡ŒåŠ¨æªæ–½â€...")
    prompt2 = prompt_template_action.format(context=context_str, query=query, finding=preliminary_finding)
    actionable_advice = generate_response(llm, llm_tokenizer, prompt2)
    print("âœ… â€œè¡ŒåŠ¨æªæ–½â€å·²ç”Ÿæˆã€‚")
    
    return preliminary_finding, actionable_advice

# --- 5. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    retriever = LawRetriever(INDEX_PATH, KNOWLEDGE_BASE_PATH, EMBEDDING_MODEL_PATH)
    llm, llm_tokenizer = load_model_with_lora(BASE_MODEL_PATH, LORA_ADAPTER_PATH)

    user_query = "æˆ‘ä¹°äº†ä¸€ä¸ªç”µè„‘ï¼Œå•†å®¶è°éª—æˆ‘å‘äº†è™šå‡çš„æ˜¾å¡ï¼Œä½†æ˜¯æˆ‘å› ä¸ºç¼ºä¹ä¸“ä¸šçŸ¥è¯†ä¸€ç›´æ²¡æœ‰å‘ç°ï¼Œ7å¤©åè¿‡äº†é€€è´§æ—¶é—´å•†å®¶å°±æ‹’ç»é€€è´§äº†ï¼Œæˆ‘åº”è¯¥æ€ä¹ˆè¦å›æˆ‘çš„é’±ï¼Ÿ"
    
    finding, actions = run_rag_chain(user_query, retriever, llm, llm_tokenizer)

    print("\n\n" + "#"*80)
    print("                 æœ€ç»ˆæ³•å¾‹å’¨è¯¢ç»“æœ")
    print("#"*80)
    print("\nã€ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆæ­¥æ–‡ä¹¦åˆ¤å†³ã€‘")
    print("-------------------------")
    print(finding)
    print("\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šå…·ä½“è¡ŒåŠ¨æªæ–½ã€‘")
    print("-------------------------")
    print(actions)
    print("\n" + "#"*80)
