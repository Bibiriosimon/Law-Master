# LAW MASTER é¡¹ç›®è¿­ä»£è®°å½•

## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

**é¡¹ç›®åç§°**ï¼šLAW MASTER - æ™ºèƒ½æ³•å¾‹åŠ©æ‰‹ç³»ç»Ÿ  
**å¼€å‘å‘¨æœŸ**ï¼š3æ¬¡é‡å¤§è¿­ä»£  
**æœ€ç»ˆéƒ¨ç½²ç¯å¢ƒ**ï¼šåä¸ºæ˜‡è…¾910Bç”Ÿæ€  
**æ ¸å¿ƒæŠ€æœ¯æ ˆ**ï¼šPyTorch 2.1.0 + DeepSeek + RAG + LoRAå¾®è°ƒ

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ä¸æ„¿æ™¯

### æ ¸å¿ƒç›®æ ‡
æ‰“é€ ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„çš„æ™ºèƒ½æ³•å¾‹åŠ©æ‰‹ç³»ç»Ÿï¼Œè®©æ™®é€šäººæ— éœ€æ”¯ä»˜é«˜é¢å’¨è¯¢è´¹ç”¨ï¼Œå³å¯é€šè¿‡è‡ªç„¶è¯­è¨€æé—®è·å¾—ä¸“ä¸šã€å‡†ç¡®çš„æ³•å¾‹è§£ç­”ã€‚

### æŠ€æœ¯æ„¿æ™¯
- âœ… å®ç°å£è¯­åŒ–é—®é¢˜çš„ç²¾å‡†ç†è§£
- âœ… æ„å»ºé«˜æ•ˆçš„æ³•å¾‹çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ
- âœ… ç”Ÿæˆä¸“ä¸šä¸”é€šä¿—æ˜“æ‡‚çš„æ³•å¾‹è§£ç­”
- âœ… åœ¨å›½äº§AIç¡¬ä»¶å¹³å°ä¸Šå®ç°å®Œå…¨è‡ªä¸»éƒ¨ç½²

---

## ğŸ”„ è¿­ä»£å†ç¨‹æ€»è§ˆ

```
ç¬¬ä¸€æ¬¡è¿­ä»£ â”€â”€â–º ç¬¬äºŒæ¬¡è¿­ä»£ â”€â”€â–º ç¬¬ä¸‰æ¬¡è¿­ä»£
  åŒæ¨¡å‹         ä¸‰æ¨¡å‹+RL        åä¸ºæ˜‡è…¾ç”Ÿæ€
  åŸºç¡€æ¶æ„       æ€§èƒ½ä¼˜åŒ–         å®Œå…¨ä½“éƒ¨ç½²
  (2ä¸ªæœˆ)        (1.5ä¸ªæœˆ)        (1ä¸ªæœˆ)
```

---

## ğŸ“– ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šåŒæ¨¡å‹åŸºç¡€æ¶æ„

### â° æ—¶é—´å‘¨æœŸ
2024å¹´8æœˆ - 2024å¹´10æœˆï¼ˆçº¦2ä¸ªæœˆï¼‰

### ğŸ¯ è¿­ä»£ç›®æ ‡
å»ºç«‹é¡¹ç›®åŸºç¡€æ¡†æ¶ï¼Œå®ç°åŸºæœ¬çš„RAGé—®ç­”èƒ½åŠ›ã€‚

### ğŸ—ï¸ æŠ€æœ¯æ¶æ„

#### 1. æ¨¡å‹é€‰å‹
- **åŸºåº§æ¨¡å‹**ï¼šDeepSeek-R1-Distill-Qwen-1.5B
  - å‚æ•°é‡ï¼š1.5Bï¼Œé€‚åˆå•å¡éƒ¨ç½²
  - æ”¯æŒä¸­æ–‡ç†è§£
  - æ¨ç†é€Ÿåº¦å¿«
  
#### 2. åŒæ¨¡å‹è®¾è®¡

**æ¨¡å‹Aï¼šæ³•å¾‹é—®ç­”ç”Ÿæˆæ¨¡å‹**
- åŠŸèƒ½ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ³•æ¡ç”Ÿæˆä¸“ä¸šè§£ç­”
- è®­ç»ƒæ•°æ®ï¼š`train_data_clean.json`ï¼ˆåˆç‰ˆæ•°æ®é›†ï¼‰
- å¾®è°ƒæ–¹æ³•ï¼šLoRA
- è®­ç»ƒè„šæœ¬ï¼š`train.py`

**æ¨¡å‹Bï¼šå‘é‡æ£€ç´¢ç³»ç»Ÿ**
- åµŒå…¥æ¨¡å‹ï¼štext2vec-base-chinese
- å‘é‡æ•°æ®åº“ï¼šFAISS IndexFlatL2
- çŸ¥è¯†åº“æ¥æºï¼š
  - ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸
  - ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•
  - ä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•
  - ä¸­åäººæ°‘å…±å’Œå›½å…¬å¸æ³•

### ğŸ“Š æ•°æ®å‡†å¤‡æµç¨‹

#### Step 1: æ³•å¾‹æ–‡æœ¬æ”¶é›†
```python
# RAG/ ç›®å½•ä¸‹æ”¶é›†æ³•å¾‹æ–‡æœ¬
- ä¸­åäººæ°‘å…±å’Œå›½å…¬å¸æ³•.txt
- ä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•.txt
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸.txt
```

#### Step 2: æ–‡æœ¬åˆ†å—å¤„ç†ï¼ˆ`rag_prepare.py`ï¼‰
```python
å…³é”®å‡½æ•°ï¼š
1. iter_files(input_dir)
   - éå†è¾“å…¥ç›®å½•ï¼ŒæŸ¥æ‰¾.txt/.json/.jsonlæ–‡ä»¶
   
2. load_file(fp)
   - è¯»å–æ–‡ä»¶å†…å®¹
   - è§£æJSON/JSONLæ ¼å¼
   - æå–textå­—æ®µ
   
3. token_chunk_with_tokenizer(text, tokenizer, chunk_size_tokens=400, overlap_tokens=80)
   - ä½¿ç”¨tokenizerè¿›è¡Œtokençº§åˆ«åˆ†å—
   - chunk_size_tokens: æ¯å—400ä¸ªtoken
   - overlap_tokens: é‡å 80ä¸ªtokenï¼Œä¿è¯è¿è´¯æ€§
   
4. char_chunk(text, chunk_size_chars=1200, overlap_chars=200)
   - å­—ç¬¦çº§åˆ«åˆ†å—ï¼ˆä½œä¸ºfallbackï¼‰
   
è¾“å‡ºï¼š
- law_chunks.jsonl: æ¯è¡Œä¸€ä¸ªchunkçš„å…ƒä¿¡æ¯
- metadata_chunks.json: chunkåˆ—è¡¨å…ƒæ•°æ®
```

#### Step 3: å‘é‡åŒ–ä¸ç´¢å¼•æ„å»º
```python
è„šæœ¬: rag_prepare.pyï¼ˆåéƒ¨åˆ†ï¼‰

å…³é”®æµç¨‹ï¼š
1. åŠ è½½åµŒå…¥æ¨¡å‹
   tokenizer = AutoTokenizer.from_pretrained(model_dir)
   model = AutoModel.from_pretrained(model_dir)
   
2. æ‰¹é‡å‘é‡åŒ–
   for batch in texts:
       embeddings = embed_with_transformers_batch(batch, tokenizer, model)
       
3. å‘é‡å½’ä¸€åŒ–
   norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
   embeddings = embeddings / norms
   
4. æ„å»ºFAISSç´¢å¼•
   index = faiss.IndexFlatIP(emb_dim)  # å†…ç§¯ç´¢å¼•ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
   index.add(embeddings.astype('float32'))
   
è¾“å‡ºï¼š
- law_index.faiss: FAISSå‘é‡ç´¢å¼•
- embeddings.npy: å‘é‡çŸ©é˜µ
```

### ğŸ“ æ¨¡å‹è®­ç»ƒ

#### è®­ç»ƒè„šæœ¬ï¼š`train.py`

```python
æ ¸å¿ƒé…ç½®ï¼š
- æ•°æ®é›†: train_data_clean.json
- åŸºåº§æ¨¡å‹: deepseek/
- è¾“å‡ºç›®å½•: ./output_deepseek_legal_lora/
- Batch Size: 4 (gradient_accumulation_steps=8, å®é™…=32)
- Learning Rate: 2e-5
- Epochs: 5
- éªŒè¯é›†æ¯”ä¾‹: 5%

LoRAé…ç½®ï¼š
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"],
    r=8,                    # LoRAç§©
    lora_alpha=32,          # ç¼©æ”¾å› å­
    lora_dropout=0.1        # Dropoutç‡
)

æ—©åœæœºåˆ¶ï¼š
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3,
    early_stopping_threshold=0.01
)
```

#### æ•°æ®é¢„å¤„ç†å‡½æ•°
```python
def process_func(example):
    MAX_LENGTH = 1024
    tokenized = tokenizer(
        example['text'],
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized
```

#### è®­ç»ƒç›‘æ§
```python
è¾“å‡ºæ£€æŸ¥ç‚¹ï¼š
- checkpoint-50, checkpoint-100, ..., checkpoint-1250
- final_model/: æœ€ç»ˆæœ€ä½³æ¨¡å‹
- training_log_history.json: è®­ç»ƒæ—¥å¿—

å…³é”®æŒ‡æ ‡ï¼š
- Training Loss: ä»2.5é™è‡³0.8
- Eval Loss: ä»2.3é™è‡³1.2
- æ€»è®­ç»ƒæ—¶é•¿: çº¦6å°æ—¶ï¼ˆA100å•å¡ï¼‰
```

### ğŸ” æ£€ç´¢ç³»ç»Ÿå®ç°

#### åŸºç¡€æ£€ç´¢å™¨ï¼ˆ`retriever.py`çš„åˆç‰ˆï¼‰
```python
class LawRetriever:
    def __init__(self, index_path, docs_path, embedding_model_path):
        # 1. åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£
        with open(docs_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        # 2. åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(index_path)
        
        # 3. åŠ è½½åµŒå…¥æ¨¡å‹
        self.embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_path)
        self.embedding_model = AutoModel.from_pretrained(embedding_model_path).to(DEVICE)
        self.embedding_model.eval()
    
    def _mean_pooling(self, model_output, attention_mask):
        """å¹³å‡æ± åŒ–"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / \
               torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def retrieve(self, query: str, k: int = 5) -> list[str]:
        """å‘é‡æ£€ç´¢"""
        # ç¼–ç æŸ¥è¯¢
        encoded_input = self.embedding_tokenizer(
            [query], 
            padding=True, 
            truncation=True, 
            max_length=512, 
            return_tensors='pt'
        ).to(DEVICE)
        
        # ç”ŸæˆåµŒå…¥
        with torch.no_grad():
            model_output = self.embedding_model(**encoded_input)
            query_embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])
            query_embedding = F.normalize(query_embedding, p=2, dim=1).cpu().numpy()
        
        # FAISSæœç´¢
        distances, indices = self.index.search(query_embedding, k)
        retrieved_docs = [self.documents[i] for i in indices[0]]
        
        return retrieved_docs
```

### ğŸ’¬ é—®ç­”æµç¨‹å®ç°ï¼ˆ`final.py`ï¼‰

```python
æ ¸å¿ƒæµç¨‹ï¼š

1. åˆå§‹åŒ–ç»„ä»¶
   retriever = LawRetriever(INDEX_PATH, KNOWLEDGE_BASE_PATH, EMBEDDING_MODEL_PATH)
   llm, llm_tokenizer = load_model_with_lora(BASE_MODEL_PATH, LORA_ADAPTER_PATH)

2. æ£€ç´¢ç›¸å…³æ³•æ¡
   retrieved_context = retriever.retrieve(query, k=5)
   context_str = "\n\n".join(retrieved_context)

3. æ„å»ºPrompt
   prompt_template_finding = """
   # è§’è‰²
   ä½ æ˜¯ä¸€åèµ„æ·±çš„ä¸­å›½æ³•å¾‹ä¸“å®¶ã€‚
   
   # ä»»åŠ¡
   ä¸¥æ ¼æ ¹æ®ä¸‹æ–‡æä¾›çš„[ç›¸å…³æ³•æ¡]ï¼Œç»“åˆ[ç”¨æˆ·é—®é¢˜]ï¼Œæ’°å†™ä¸“ä¸šçš„æ³•å¾‹åˆ†æã€‚
   
   [ç›¸å…³æ³•æ¡]
   {context}
   
   [ç”¨æˆ·é—®é¢˜]
   {query}
   
   # è¾“å‡ºæŠ¥å‘Š
   [ä½ çš„åˆ†æå’Œæ–‡ä¹¦åˆ¤å†³]
   """

4. ç”Ÿæˆç­”æ¡ˆ
   prompt1 = prompt_template_finding.format(context=context_str, query=query)
   preliminary_finding = generate_response(llm, llm_tokenizer, prompt1)

5. ç”Ÿæˆè¡ŒåŠ¨å»ºè®®ï¼ˆåŒé˜¶æ®µç”Ÿæˆï¼‰
   prompt_template_action = """
   åŸºäºå·²æœ‰çš„[åˆæ­¥çš„æ–‡ä¹¦åˆ¤å†³]å’Œ[ç›¸å…³æ³•æ¡]ï¼Œæä¾›æ¸©æš–ã€æ¸…æ™°çš„è¡ŒåŠ¨æŒ‡å—ã€‚
   ...
   """
   actionable_advice = generate_response(llm, llm_tokenizer, prompt2)
```

### ğŸ“ˆ ç¬¬ä¸€æ¬¡è¿­ä»£æˆæœ

#### æˆåŠŸç‚¹
âœ… å»ºç«‹äº†å®Œæ•´çš„RAGé—®ç­”æµæ°´çº¿  
âœ… å®ç°äº†å‘é‡æ£€ç´¢ç³»ç»Ÿ  
âœ… å®Œæˆäº†æ³•å¾‹é—®ç­”æ¨¡å‹çš„å¾®è°ƒ  
âœ… èƒ½å¤Ÿç”ŸæˆåŸºæœ¬å¯ç”¨çš„æ³•å¾‹è§£ç­”  

#### å­˜åœ¨é—®é¢˜
âŒ **æ£€ç´¢å‡†ç¡®ç‡ä¸è¶³**ï¼šçº¯å‘é‡æ£€ç´¢å¬å›ç‡ä»…çº¦30%  
âŒ **å£è¯­åŒ–ç†è§£å¼±**ï¼šç”¨æˆ·æé—®éœ€è¦è¾ƒè§„èŒƒæ‰èƒ½æ£€ç´¢å‡†ç¡®  
âŒ **ç­”æ¡ˆæ³›åŒ–æ€§å·®**ï¼šå¯¹è®­ç»ƒé›†å¤–çš„é—®é¢˜å›ç­”è´¨é‡ä¸‹é™  
âŒ **ç¼ºä¹è¯„æµ‹ä½“ç³»**ï¼šæ²¡æœ‰ç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°æ–¹æ³•  

---

## ğŸš€ ç¬¬äºŒæ¬¡è¿­ä»£ï¼šä¸‰æ¨¡å‹æ¶æ„+æ··åˆæ£€ç´¢

### â° æ—¶é—´å‘¨æœŸ
2024å¹´10æœˆ - 2024å¹´11æœˆï¼ˆçº¦1.5ä¸ªæœˆï¼‰

### ğŸ¯ è¿­ä»£ç›®æ ‡
- è§£å†³å£è¯­åŒ–é—®é¢˜ç†è§£ä¸è¶³çš„é—®é¢˜
- æå‡æ£€ç´¢å‡†ç¡®ç‡
- å»ºç«‹ç³»ç»Ÿæ€§è¯„æµ‹ä½“ç³»
- ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆè´¨é‡

### ğŸ—ï¸ æ¶æ„å‡çº§

#### æ ¸å¿ƒåˆ›æ–°ï¼šä¸‰æ¨¡å‹ååŒ

```
ç”¨æˆ·è¾“å…¥ 
  â†“
[æ¨¡å‹1: æŸ¥è¯¢é‡å†™SLM] â† æ–°å¢
  â†“ è¾“å‡ºç»“æ„åŒ–æŸ¥è¯¢
[æ¨¡å‹2: æ··åˆæ£€ç´¢ç³»ç»Ÿ] â† å‡çº§
  â†“ å¬å›ç›¸å…³æ³•æ¡
[æ¨¡å‹3: ç­”æ¡ˆç”ŸæˆSLM] â† åŸæœ‰
  â†“
ä¸“ä¸šæ³•å¾‹è§£ç­”
```

### ğŸ“Š æ•°æ®é›†æ„å»º

#### 1. æŸ¥è¯¢é‡å†™æ•°æ®é›†ç”Ÿæˆï¼ˆ`SLM_DATASET.py`ï¼‰

```python
ç›®æ ‡ï¼šç”Ÿæˆ1000+æ¡é«˜è´¨é‡çš„æŸ¥è¯¢é‡å†™æ ·æœ¬

ä¸‰é˜¶æ®µç”Ÿæˆæµç¨‹ï¼š

é˜¶æ®µ1: ç”Ÿæˆæ³•å¾‹ä¸»é¢˜
def generate_topics_for_law(law_name):
    prompt = f"è¯·é’ˆå¯¹ã€Š{law_name}ã€‹ï¼Œæ„æ€å‡º {TOPICS_PER_LAW} ä¸ªæ™®é€šæ°‘ä¼—æœ€å…³å¿ƒçš„æ ¸å¿ƒæ³•å¾‹ä¸»é¢˜ã€‚"
    # è°ƒç”¨DeepSeek API
    # è¿”å›ä¸»é¢˜åˆ—è¡¨

è¾“å…¥: LAW_LIST = ["åˆ‘æ³•", "æ°‘æ³•å…¸", "å…¬å¸æ³•", ...]
è¾“å‡º: 13éƒ¨æ³•å¾‹ Ã— 15ä¸ªä¸»é¢˜/æ³•å¾‹ = 195ä¸ªä¸»é¢˜

é˜¶æ®µ2: æ‰¹é‡ç”Ÿæˆå£è¯­åŒ–é—®é¢˜
def batch_generate_questions(topic_batch_with_laws):
    # æ¯ä¸ªä¸»é¢˜ç”Ÿæˆ5ä¸ªå£è¯­åŒ–é—®é¢˜
    # ä½¿ç”¨JSON modeç¡®ä¿ç»“æ„åŒ–è¾“å‡º
    return questions_list

è¾“å‡º: 195ä¸»é¢˜ Ã— 5é—®é¢˜/ä¸»é¢˜ = 975ä¸ªå£è¯­åŒ–é—®é¢˜

é˜¶æ®µ3: æ‰¹é‡è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
def batch_create_training_samples(question_batch):
    prompt = """
    è¯·åˆ†æä»¥ä¸‹æ³•å¾‹é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸€ä¸ªç”Ÿæˆç»“æ„åŒ–JSON:
    {
      "keywords_for_search": ["å…³é”®è¯1", "å…³é”®è¯2", ...],
      "query_for_vector_search": "ä¹¦é¢åŒ–æŸ¥è¯¢"
    }
    """
    return training_samples

è¾“å‡º: synthetic_query_rewriter_dataset_robust_1k.jsonl
æ ¼å¼:
{
  "instruction": "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æŸ¥è¯¢åŠ©æ‰‹...",
  "input": "è€æ¿æ‹–æ¬ å·¥èµ„ä¸ç»™ï¼Œæ€ä¹ˆåŠï¼Ÿ",
  "output": "{\"keywords_for_search\": [\"åŠ³åŠ¨æŠ¥é…¬\", \"æ‹–æ¬ å·¥èµ„\"], \"query_for_vector_search\": \"ç”¨äººå•ä½æ‹–æ¬ å·¥èµ„çš„æ³•å¾‹è´£ä»»\"}"
}
```

**å¹¶å‘ä¼˜åŒ–**ï¼š
- ä½¿ç”¨ `ThreadPoolExecutor` å¹¶å‘å¤„ç†ï¼ˆMAX_WORKERS=8ï¼‰
- æ‰¹é‡APIè°ƒç”¨å‡å°‘ç½‘ç»œå¼€é”€
- ç”Ÿæˆé€Ÿåº¦ï¼š~1000æ ·æœ¬/10åˆ†é’Ÿ

---

## 5. æ¨¡å‹è®­ç»ƒè¯¦è§£

### 5.1 è®­ç»ƒç¯å¢ƒé…ç½®

**ç¡¬ä»¶å¹³å°**ï¼š
```yaml
GPU: NVIDIA A100 40GB (AutoDLå¹³å°) / Huawei Ascend 910B
CPU: 16 vCPU
å†…å­˜: 60GB
å­˜å‚¨: 200GB SSD
```

**è½¯ä»¶ç¯å¢ƒ**ï¼š
```bash
PyTorch: 2.1.0
Transformers: 4.36.0
PEFT: 0.7.0
CUDA: 11.8 (NVIDIA) / CANN 8.1.RC1 (Ascend)
Python: 3.11
```

### 5.2 è®­ç»ƒè„šæœ¬è¯¦è§£ - ä¸»ç”Ÿæˆæ¨¡å‹ (train.py)

#### æ ¸å¿ƒé…ç½®

```python
# æ•°æ®é›†
DATA_FILE = "dataset_final_moreversion.jsonl"
dataset = load_dataset('json', data_files=DATA_FILE, split='train')

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
dataset = dataset.train_test_split(test_size=0.05, seed=42)  # 95% è®­ç»ƒ, 5% éªŒè¯

# æ¨¡å‹è·¯å¾„
model_name_or_path = '/root/autodl-tmp/legal_finetune/deepseek'

# LoRAé…ç½®
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"],  # å…¨æ³¨æ„åŠ›å±‚
    r=8,                      # LoRAç§©
    lora_alpha=32,           # ç¼©æ”¾å› å­
    lora_dropout=0.1         # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
)

# è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir="./output_deepseek_legal_lora_v2",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,      # æœ‰æ•ˆbatch_size = 4Ã—8 = 32
    learning_rate=2e-5,
    num_train_epochs=5,
    save_steps=50,
    eval_steps=50,
    evaluation_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    gradient_checkpointing=True         # èŠ‚çœæ˜¾å­˜
)

# æ—©åœå›è°ƒ
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3,
    early_stopping_threshold=0.01
)
```

#### è®­ç»ƒæµç¨‹

1. **æ•°æ®é¢„å¤„ç†**ï¼š
```python
def process_func(example):
    MAX_LENGTH = 1024
    tokenized = tokenizer(
        example['text'],
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized
```

2. **è®­ç»ƒæ‰§è¡Œ**ï¼š
```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    data_collator=data_collator,
    callbacks=[early_stopping_callback]
)

trainer.train()
```

3. **è®­ç»ƒç›‘æ§**ï¼š
- æ¯50æ­¥ä¿å­˜æ£€æŸ¥ç‚¹
- æ¯50æ­¥åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- LossæŒç»­3æ¬¡æœªæ”¹å–„åˆ™æ—©åœ
- æœ€ç»ˆåŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹

### 5.3 è®­ç»ƒè„šæœ¬è¯¦è§£ - æŸ¥è¯¢é‡å†™æ¨¡å‹ (SLM_TRAIN.py)

#### å…³é”®å·®å¼‚

```python
# ä¸“ç”¨æ•°æ®é›†
DATA_FILE = "synthetic_query_rewriter_dataset_robust_1k.jsonl"

# æ›´å°çš„max_lengthï¼ˆæŸ¥è¯¢é‡å†™ä»»åŠ¡æ›´çŸ­ï¼‰
MAX_LENGTH = 768

# Promptæ¨¡æ¿
prompt_template = (
    "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æŸ¥è¯¢åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æ³•å¾‹é—®é¢˜ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONå¯¹è±¡ï¼Œ"
    "åŒ…å«ç”¨äºå…³é”®è¯æœç´¢çš„'keywords_for_search'å’Œç”¨äºå‘é‡æœç´¢çš„'query_for_vector_search'ã€‚\n\n"
    "### ç”¨æˆ·é—®é¢˜:\n{input}\n\n### JSONè¾“å‡º:\n{output}"
)

# æ•°æ®æ ¼å¼åŒ–
def format_and_tokenize(example):
    text = prompt_template.format(
        input=example['input'],
        output=example['output']
    ) + tokenizer.eos_token
    
    tokenized = tokenizer(text, max_length=768, truncation=True, padding="max_length")
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized
```

#### è®­ç»ƒç»“æœ

```
è¾“å‡ºç›®å½•: ./output_query_rewriter_lora/
æœ€ä½³æ£€æŸ¥ç‚¹: checkpoint-130/
éªŒè¯é›†Loss: 0.234
è®­ç»ƒæ—¶é—´: ~2å°æ—¶ (A100 40GB)
```

### 5.4 è®­ç»ƒç›‘æ§ä¸å¯è§†åŒ–

**Lossæ›²çº¿ç»˜åˆ¶** (plot.py / plot2.py):

```python
import json
import matplotlib.pyplot as plt

# åŠ è½½è®­ç»ƒæ—¥å¿—
with open('output_deepseek_legal_lora_v2/training_log_history.json', 'r') as f:
    log_history = json.load(f)

# æå–è®­ç»ƒå’ŒéªŒè¯loss
train_loss = [x['loss'] for x in log_history if 'loss' in x]
eval_loss = [x['eval_loss'] for x in log_history if 'eval_loss' in x]

# ç»˜åˆ¶æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(train_loss, label='Training Loss')
plt.plot(eval_loss, label='Validation Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Progress')
plt.legend()
plt.savefig('loss_curve.png', dpi=300)
```

**ç»“æœç¤ºä¾‹**ï¼š
- è®­ç»ƒé›†Lossä»2.5é™è‡³0.8
- éªŒè¯é›†Lossä»2.3é™è‡³1.1
- æ— æ˜æ˜¾è¿‡æ‹Ÿåˆè¿¹è±¡

---

## 6. RAGçŸ¥è¯†åº“æ„å»º

### 6.1 æ•°æ®å‡†å¤‡ (rag_prepare.py)

#### è¾“å…¥æ•°æ®

```
RAG/
â”œâ”€â”€ ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸.txt
â”œâ”€â”€ ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•.txt
â””â”€â”€ ä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•.txt
```

#### æ ¸å¿ƒåŠŸèƒ½

**1. æ™ºèƒ½åˆ†å—**ï¼š
```python
def token_chunk_with_tokenizer(text, tokenizer, chunk_size_tokens=400, overlap_tokens=80):
    """æŒ‰tokenæ•°é‡åˆ†å—ï¼Œç¡®ä¿è¯­ä¹‰å®Œæ•´"""
    enc = tokenizer(text, add_special_tokens=False)
    ids = enc['input_ids']
    total = len(ids)
    
    chunks = []
    i = 0
    while i < total:
        j = min(i + chunk_size_tokens, total)
        chunk_ids = ids[i:j]
        decoded = tokenizer.decode(chunk_ids)
        chunks.append((i, j, decoded))
        
        if j >= total:
            break
        i = max(0, j - overlap_tokens)  # æ»‘åŠ¨çª—å£
    
    return chunks
```

**2. æŒ‰æ¡æ–‡åˆ†å‰²**ï¼š
```python
# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«"ç¬¬Xæ¡"
parts = re.split(r'(?=(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒ\d]+æ¡))', text)

# åˆå¹¶æ ‡é¢˜ä¸å†…å®¹
merged = []
i = 0
while i < len(parts):
    if re.match(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒ\d]+æ¡', parts[i]):
        heading = parts[i]
        body = parts[i+1] if i+1 < len(parts) else ''
        merged.append((heading + body).strip())
        i += 2
    else:
        merged.append(parts[i].strip())
        i += 1
```

**3. å‘é‡åŒ–**ï¼š
```python
# ä½¿ç”¨Sentence-Transformersæˆ–Transformersç”ŸæˆåµŒå…¥
if use_sentence_transformer:
    st_model = SentenceTransformer(model_dir, device=device)
    embeddings = st_model.encode(
        texts, 
        batch_size=32, 
        show_progress_bar=True, 
        convert_to_numpy=True
    )
else:
    embeddings = embed_with_transformers_batch(
        texts, tokenizer, model, device, batch_size=16
    )

# L2å½’ä¸€åŒ–
norms = np.linalg.norm(embeddings, axis=1, keepdims=True).clip(min=1e-9)
embeddings = embeddings / norms
```

**4. FAISSç´¢å¼•æ„å»º**ï¼š
```python
import faiss

emb_dim = embeddings.shape[1]  # é€šå¸¸æ˜¯768æˆ–1024
index = faiss.IndexFlatIP(emb_dim)  # å†…ç§¯ç´¢å¼•ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
index.add(embeddings.astype('float32'))

# ä¿å­˜
faiss.write_index(index, "law_index.faiss")
```

#### è¾“å‡ºæ–‡ä»¶

```
rag_store/
â”œâ”€â”€ law_chunks.jsonl          # æ¯è¡Œä¸€ä¸ªchunkçš„å…ƒä¿¡æ¯
â”œâ”€â”€ metadata_chunks.json       # chunkå…ƒæ•°æ®
â”œâ”€â”€ law_index.faiss           # FAISSå‘é‡ç´¢å¼•
â””â”€â”€ embeddings.npy            # åŸå§‹å‘é‡çŸ©é˜µ
```

### 6.2 å‡è®¾æ€§é—®é¢˜å¢å¼º (HyDEæŠ€æœ¯)

**æ¦‚å¿µ**ï¼šä¸ºæ¯ä¸ªæ³•æ¡ç”Ÿæˆå¤šä¸ªå‡è®¾æ€§ç”¨æˆ·é—®é¢˜ï¼Œå¢å¼ºæ£€ç´¢å¬å›å¤šæ ·æ€§ã€‚

**å®ç°è„šæœ¬** (éƒ¨åˆ†ä¼ªä»£ç ):
```python
# ä¸ºæ¯ä¸ªæ³•æ¡ç”Ÿæˆ3-5ä¸ªå‡è®¾æ€§é—®é¢˜
def generate_hypothetical_questions(law_text, article_number):
    prompt = f"""
    ä¸‹é¢æ˜¯ä¸€æ¡æ³•å¾‹æ¡æ–‡ï¼š
    
    {article_number}
    {law_text}
    
    è¯·ç”Ÿæˆ3-5ä¸ªæ™®é€šäººå¯èƒ½ä¼šé—®çš„ã€ä¸è¿™æ¡æ³•å¾‹ç›¸å…³çš„å£è¯­åŒ–é—®é¢˜ã€‚
    """
    
    response = call_deepseek_api(prompt)
    questions = parse_questions(response)
    return questions

# å¢å¼ºåçš„chunkæ ¼å¼
enriched_chunk = {
    "id": "chunk_123",
    "article_number": "æ°‘æ³•å…¸ç¬¬ä¸€åƒä¸€ç™¾å…­åäº”æ¡",
    "content": "è¡Œä¸ºäººå› è¿‡é”™ä¾µå®³ä»–äººæ°‘äº‹æƒç›Šé€ æˆæŸå®³çš„...",
    "hypothetical_questions": [
        "è¢«åˆ«äººæ‰“ä¼¤äº†ï¼Œå¯¹æ–¹è¦èµ”å¿å—ï¼Ÿ",
        "å¼€è½¦æ’åˆ°äººï¼Œéœ€è¦æ‰¿æ‹…ä»€ä¹ˆè´£ä»»ï¼Ÿ",
        ...
    ],
    "keywords": ["ä¾µæƒè´£ä»»", "è¿‡é”™", "èµ”å¿"]
}
```

**æ•ˆæœ**ï¼š
- æ£€ç´¢å¬å›ç‡æå‡60%+
- æ”¯æŒæ›´å£è¯­åŒ–çš„æŸ¥è¯¢
- å¢å¼ºè¯­ä¹‰åŒ¹é…èƒ½åŠ›

### 6.3 å‘é‡æ•°æ®åº“æ„å»º (vector.py)

#### å®Œæ•´æµç¨‹

```python
# 1. åŠ è½½å¢å¼ºåçš„è¯­æ–™åº“
chunks = load_enriched_corpus('corpus_enriched_fast.jsonl')

# 2. å‡†å¤‡åµŒå…¥æ–‡æœ¬ï¼ˆåˆå¹¶å‡è®¾æ€§é—®é¢˜ä¸åŸæ–‡ï¼‰
texts_to_embed = []
for chunk in chunks:
    questions_str = "\n".join(chunk.get("hypothetical_questions", []))
    combined_text = f"ç›¸å…³é—®é¢˜ï¼š\n{questions_str}\n\næ³•å¾‹æ¡æ–‡ï¼š\n{chunk['content']}"
    texts_to_embed.append(combined_text)

# 3. æ‰¹é‡å‘é‡åŒ–
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)
model = AutoModel.from_pretrained(LOCAL_MODEL_PATH).to(DEVICE)
model.eval()

all_embeddings = []
for i in tqdm(range(0, len(texts_to_embed), BATCH_SIZE)):
    batch_texts = texts_to_embed[i:i + BATCH_SIZE]
    encoded_input = tokenizer(batch_texts, padding=True, truncation=True, 
                              max_length=512, return_tensors='pt').to(DEVICE)
    
    with torch.no_grad():
        model_output = model(**encoded_input)
    
    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
    normalized_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
    all_embeddings.append(normalized_embeddings.cpu().numpy())

embeddings = np.vstack(all_embeddings).astype('float32')

# 4. æ„å»ºFAISSç´¢å¼•ï¼ˆå¸¦IDæ˜ å°„ï¼‰
d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)
ids = np.arange(len(chunks))
index = faiss.IndexIDMap(index)
index.add_with_ids(embeddings, ids)

# 5. ä¿å­˜
faiss.write_index(index, 'law_enhanced_vector_db.faiss')

# 6. ä¿å­˜IDåˆ°chunkçš„æ˜ å°„
index_to_chunk_map = {i: chunk for i, chunk in enumerate(chunks)}
with open('index_to_chunk_map.json', 'w', encoding='utf-8') as f:
    json.dump(index_to_chunk_map, f, ensure_ascii=False, indent=4)
```

---

## 7. æ··åˆæ£€ç´¢ç³»ç»Ÿå®ç°

### 7.1 ç³»ç»Ÿè®¾è®¡ (retriever.py / hybrid_example.py)

#### HybridRetrieverç±»æ¶æ„

```python
class HybridRetriever:
    def __init__(self, embedding_model_path, faiss_index_path, chunk_map_path):
        # 1. åŠ è½½åµŒå…¥æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_path)
        self.embedding_model = AutoModel.from_pretrained(embedding_model_path).to(self.device)
        self.embedding_model.eval()
        
        # 2. åŠ è½½FAISSç´¢å¼•
        self.faiss_index = faiss.read_index(faiss_index_path)
        
        # 3. åŠ è½½chunkæ˜ å°„
        with open(chunk_map_path, 'r', encoding='utf-8') as f:
            self.chunk_map = {int(k): v for k, v in json.load(f).items()}
        
        # 4. æ„å»ºå€’æ’ç´¢å¼•ï¼ˆç”¨äºå…³é”®è¯æœç´¢ï¼‰
        self.inverted_index = self._build_inverted_index()
    
    def _build_inverted_index(self):
        """æ„å»ºå…³é”®è¯å€’æ’ç´¢å¼•"""
        inverted_index = {}
        for doc_id, chunk_data in self.chunk_map.items():
            for keyword in chunk_data.get('keywords', []):
                if keyword not in inverted_index:
                    inverted_index[keyword] = []
                inverted_index[keyword].append(doc_id)
        return inverted_index
```

#### å‘é‡æœç´¢

```python
def _vector_search(self, query_text, k=5):
    """æ‰§è¡Œå‘é‡è¯­ä¹‰æœç´¢"""
    # 1. å°†æŸ¥è¯¢å‘é‡åŒ–
    query_vector = self._encode([query_text]).cpu().numpy().astype('float32')
    
    # 2. FAISSæœç´¢
    distances, ids = self.faiss_index.search(query_vector, k)
    
    # 3. è¿”å›(id, score)åˆ—è¡¨
    results = []
    for i in range(len(ids[0])):
        if ids[0][i] != -1:
            results.append((ids[0][i], 1 - distances[0][i]))
    return results
```

#### å…³é”®è¯æœç´¢

```python
def _keyword_search(self, keywords, k=10):
    """æ‰§è¡Œå…³é”®è¯ç²¾ç¡®æœç´¢"""
    if not keywords:
        return []
    
    # 1. ä»å€’æ’ç´¢å¼•ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
    doc_ids = []
    for kw in keywords:
        if kw in self.inverted_index:
            doc_ids.extend(self.inverted_index[kw])
    
    # 2. ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„å…³é”®è¯å‘½ä¸­æ¬¡æ•°
    id_counts = Counter(doc_ids)
    
    # 3. è¿”å›æŒ‰å‘½ä¸­æ¬¡æ•°æ’åºçš„ç»“æœ
    return [(doc_id, score) for doc_id, score in id_counts.most_common(k)]
```

#### ç»“æœèåˆä¸é‡æ’åº

```python
def _fuse_and_rerank(self, vector_results, keyword_results, 
                     keyword_weight=0.7, vector_weight=0.3):
    """èåˆä¸¤è·¯æœç´¢ç»“æœ"""
    fused_scores = {}
    
    # åŠ æƒèåˆ
    for doc_id, score in keyword_results:
        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score * keyword_weight
    
    for doc_id, score in vector_results:
        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score * vector_weight
    
    # æŒ‰æ€»åˆ†æ’åº
    sorted_results = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_id for doc_id, score in sorted_results]
```

**æƒé‡é…ç½®è¯´æ˜**ï¼š
- å…³é”®è¯æƒé‡0.7ï¼šæ³•å¾‹é—®é¢˜ä¸­ä¸“ä¸šæœ¯è¯­åŒ¹é…è‡³å…³é‡è¦
- å‘é‡æƒé‡0.3ï¼šè¯­ä¹‰ç†è§£ä½œä¸ºè¡¥å……ï¼Œæ•è·éšå«å…³è”

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæœ€ç»ˆæ¨ç†æµç¨‹æ•´åˆ

### 7.1 ç«¯åˆ°ç«¯RAGé—®ç­”æµç¨‹

æ–‡ä»¶ï¼š`final.py`, `showcase.py`

#### å®Œæ•´æµç¨‹å®ç°

```python
def run_rag_chain(query, retriever, llm, llm_tokenizer):
    """æ‰§è¡Œå®Œæ•´çš„RAGé—®ç­”é“¾"""
    
    # æ­¥éª¤1: æ£€ç´¢ç›¸å…³æ³•æ¡
    print(f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜: {query}")
    retrieved_context = retriever.retrieve(query, k=5)
    context_str = "\n\n".join(retrieved_context)
    
    # æ­¥éª¤2: ç”Ÿæˆåˆæ­¥åˆ¤å†³ï¼ˆæ–‡ä¹¦åˆ¤å†³ï¼‰
    prompt_finding = f"""
    # è§’è‰²
    ä½ æ˜¯ä¸€åèµ„æ·±çš„ä¸­å›½æ³•å¾‹ä¸“å®¶ï¼Œä»»åŠ¡æ˜¯èµ·è‰ä¸“ä¸šçš„æ³•å¾‹æ¡ˆä»¶åˆæ­¥åˆ†ææŠ¥å‘Šã€‚
    
    # ä»»åŠ¡
    ä¸¥æ ¼æ ¹æ®ä¸‹æ–‡æä¾›çš„[ç›¸å…³æ³•æ¡]ï¼Œç»“åˆ[ç”¨æˆ·é—®é¢˜]ï¼Œæ’°å†™ç»“æ„åŒ–çš„"æ–‡ä¹¦åˆ¤å†³"ã€‚
    
    [ç›¸å…³æ³•æ¡]
    {context_str}
    
    [ç”¨æˆ·é—®é¢˜]
    {query}
    
    [ä½ çš„åˆ†æå’Œæ–‡ä¹¦åˆ¤å†³]
    """
    
    preliminary_finding = generate_response(llm, llm_tokenizer, prompt_finding)
    
    # æ­¥éª¤3: ç”Ÿæˆè¡ŒåŠ¨æªæ–½ï¼ˆå‹å¥½å»ºè®®ï¼‰
    prompt_action = f"""
    # è§’è‰²
    ä½ æ˜¯ä¸€ä½å……æ»¡äººæƒ…å‘³ä¸”ç»éªŒä¸°å¯Œçš„æ³•å¾‹æ´åŠ©é¡¾é—®ã€‚
    
    # ä»»åŠ¡
    åŸºäºå·²æœ‰çš„[åˆæ­¥çš„æ–‡ä¹¦åˆ¤å†³]å’Œ[ç›¸å…³æ³•æ¡]ï¼Œä¸ºç”¨æˆ·æä¾›æ¸©æš–ã€æ¸…æ™°çš„è¡ŒåŠ¨æŒ‡å—ã€‚
    
    [ç›¸å…³æ³•æ¡]
    {context_str}
    
    [ç”¨æˆ·é—®é¢˜]
    {query}
    
    [åˆæ­¥çš„æ–‡ä¹¦åˆ¤å†³]
    {preliminary_finding}
    
    [ä½ çš„æ€è€ƒå’Œè¡ŒåŠ¨æªæ–½]
    """
    
    actionable_advice = generate_response(llm, llm_tokenizer, prompt_action)
    
    return preliminary_finding, actionable_advice
```

#### åŒé˜¶æ®µç”Ÿæˆç­–ç•¥

**é˜¶æ®µ1ï¼šæ–‡ä¹¦åˆ¤å†³**
- è§’è‰²å®šä½ï¼šèµ„æ·±æ³•å¾‹ä¸“å®¶
- è¾“å‡ºé£æ ¼ï¼šä¸“ä¸šã€ä¸¥è°¨ã€å¼•ç”¨æ³•æ¡
- è¾“å‡ºç»“æ„ï¼š
  - äº‹å®æ¢³ç†
  - æ³•å¾‹é€‚ç”¨åˆ†æ
  - åˆæ­¥ç»“è®º

**é˜¶æ®µ2ï¼šè¡ŒåŠ¨æªæ–½**
- è§’è‰²å®šä½ï¼šæ³•å¾‹æ´åŠ©é¡¾é—®
- è¾“å‡ºé£æ ¼ï¼šæ¸©æš–ã€é€šä¿—ã€é¼“åŠ±æ€§
- è¾“å‡ºç»“æ„ï¼š
  - å®‰æŠšå’Œå…±æƒ…
  - è§£è¯»åˆ¤å†³
  - æ¸…æ™°çš„ç»´æƒè·¯å¾„

### 7.2 ä¸‰æ¨¡å‹ååŒå·¥ä½œæµ

æ–‡ä»¶ï¼š`showcase.py`

```python
# æ¨¡å‹åŠ è½½
rewriter_model, rewriter_tokenizer = load_slm(BASE_MODEL_PATH, QUERY_REWRITER_SLM_PATH)
generator_model, generator_tokenizer = load_slm(BASE_MODEL_PATH, GENERATION_SLM_ADAPTER_PATH)
retriever = HybridRetriever(EMBEDDING_MODEL_PATH, FAISS_INDEX_PATH, CHUNK_MAP_PATH)

# æ‰§è¡Œæµç¨‹
def run_rag_qa_pipeline(query, rewriter_model, rewriter_tokenizer, 
                        generator_model, generator_tokenizer, retriever):
    
    # æ­¥éª¤1: æŸ¥è¯¢é‡å†™
    rewritten_data = rewrite_query_with_slm(query, rewriter_model, rewriter_tokenizer)
    # è¾“å‡º: {"keywords_for_search": [...], "query_for_vector_search": "..."}
    
    # æ­¥éª¤2: æ··åˆæ£€ç´¢
    retrieved_ids = retriever.search(
        query_for_vector=rewritten_data["query_for_vector_search"],
        keywords_for_search=rewritten_data["keywords_for_search"],
        top_k=5
    )
    context = retriever.assemble_context(retrieved_ids)
    
    # æ­¥éª¤3: ç­”æ¡ˆç”Ÿæˆ
    final_answer = generate_final_answer(query, context, generator_model, generator_tokenizer)
    
    return final_answer, context
```

### 7.3 ä¼˜åŒ–çš„Promptå·¥ç¨‹

#### æŸ¥è¯¢é‡å†™Prompt

```python
prompt_template = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„æ³•å¾‹æŸ¥è¯¢åˆ†æå¼•æ“ã€‚

### ä»»åŠ¡è¦æ±‚:
1. **åˆ†æç”¨æˆ·é—®é¢˜**: æ·±å…¥ç†è§£ç”¨æˆ·çš„æ ¸å¿ƒæ³•å¾‹è¯‰æ±‚ã€‚
2. **æå–å…³é”®è¯**: åœ¨`keywords_for_search`å­—æ®µä¸­ï¼Œæç‚¼3-5ä¸ªæ ¸å¿ƒæ³•å¾‹æœ¯è¯­ã€‚
3. **æ”¹å†™å‘é‡æŸ¥è¯¢**: åœ¨`query_for_vector_search`å­—æ®µä¸­ï¼Œå°†åŸé—®é¢˜æ”¹å†™æˆä¹¦é¢åŒ–æŸ¥è¯¢ã€‚
4. **ä¸¥æ ¼éµå¾ªæ ¼å¼**: è¾“å‡ºå¿…é¡»æ˜¯çº¯ç²¹çš„JSONå¯¹è±¡ã€‚

### ç”¨æˆ·é—®é¢˜:
{input}

### è¾“å‡ºJSON:
"""
```

#### ç­”æ¡ˆç”ŸæˆPrompt

```python
system_prompt = """
ä½ æ˜¯ä¸€åé¡¶çº§çš„ä¸­å›½æ³•å¾‹AIä¸“å®¶ã€‚

### å›ç­”è§„åˆ™:
1. **ç»“æ„æ¸…æ™°**: åˆ†ç‚¹é˜è¿°ï¼Œé¦–å…ˆç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜ï¼Œç„¶åè§£é‡Šæ³•å¾‹ä¾æ®ï¼Œæœ€åç»™å‡ºå»ºè®®ã€‚
2. **è¯­è¨€è¦æ±‚**: ä½¿ç”¨æ¸…æ™°ã€å‡†ç¡®çš„ç®€ä½“ä¸­æ–‡ã€‚ç¦æ­¢ä½¿ç”¨ä»»ä½•è‹±æ–‡å•è¯ã€‚
3. **ä¿æŒå®¢è§‚**: ä¿æŒä¸­ç«‹å’Œå®¢è§‚ï¼Œä¸æä¾›æŠ•æœºæ€§å»ºè®®ã€‚

å‚è€ƒæ³•æ¡ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{query}
"""
```

---

## ç¬¬å…«éƒ¨åˆ†ï¼šæµ‹è¯•ä¸è¯„ä¼°

### 8.1 RAGæ£€ç´¢ç­–ç•¥å¯¹æ¯”å®éªŒ

æ–‡ä»¶ï¼š`evaluation_searchcomparison.py`, `test_report.md`, `final_test_report.md`

#### å®éªŒè®¾è®¡

**æµ‹è¯•æ•°æ®**ï¼š
- é—®é¢˜æ•°é‡ï¼š30ä¸ªå£è¯­åŒ–æ³•å¾‹é—®é¢˜
- æ ‡å‡†ç­”æ¡ˆï¼šç”±LLMç”Ÿæˆçš„æ ‡å‡†æ³•æ¡ï¼ˆæ€»è®¡136ä¸ªæ³•æ¡ï¼‰
- è¯„ä¼°æŒ‡æ ‡ï¼šTop-3å‘½ä¸­ç‡

**å¯¹æ¯”ç­–ç•¥**ï¼š
1. åŸæ–‡+å…³é”®è¯
2. SLM+å…³é”®è¯
3. SLM+å‘é‡
4. SLM+æ··åˆ
5. åŸæ–‡+å‘é‡

#### æ ¸å¿ƒè¯„ä¼°ä»£ç 

```python
def evaluate_retrieval_strategy(query, ground_truth_articles, retriever, strategy):
    """è¯„ä¼°å•ä¸ªæ£€ç´¢ç­–ç•¥"""
    
    if strategy == "åŸæ–‡+å‘é‡":
        # ç›´æ¥ä½¿ç”¨åŸé—®é¢˜åšå‘é‡æœç´¢
        results = retriever._vector_search(query, k=3)
    
    elif strategy == "SLM+å‘é‡":
        # ä½¿ç”¨SLMé‡å†™åçš„å‘é‡æŸ¥è¯¢
        rewritten = rewrite_query(query)
        results = retriever._vector_search(rewritten["query_for_vector_search"], k=3)
    
    elif strategy == "SLM+æ··åˆ":
        # ä½¿ç”¨SLMé‡å†™åçš„æ··åˆæ£€ç´¢
        rewritten = rewrite_query(query)
        results = retriever.search(
            query_for_vector=rewritten["query_for_vector_search"],
            keywords_for_search=rewritten["keywords_for_search"],
            top_k=3
        )
    
    # è®¡ç®—å‘½ä¸­ç‡
    retrieved_articles = [chunk_map[id]["article_number"] for id in results]
    hits = len(set(retrieved_articles) & set(ground_truth_articles))
    
    return hits, len(retrieved_articles)
```

#### å®éªŒç»“æœ

| ç­–ç•¥ | æ€»å‘½ä¸­æ•° | æ€»å¬å›æ•° | **å‘½ä¸­ç‡** | ç›¸å¯¹æå‡ |
|------|---------|---------|-----------|---------|
| åŸæ–‡+å…³é”®è¯ | 11 | 56 | 19.64% | baseline |
| SLM+å…³é”®è¯ | 3 | 42 | 7.14% | -63.6% |
| SLM+å‘é‡ | 22 | 90 | 24.44% | +24.4% |
| **SLM+æ··åˆ** | **20** | **90** | **22.22%** | **+13.1%** |
| **åŸæ–‡+å‘é‡** | **37** | **90** | **41.11%** | **+109.3%** ğŸ† |

**å…³é”®å‘ç°**ï¼š
1. å‘é‡æœç´¢æ˜¾è‘—ä¼˜äºçº¯å…³é”®è¯æœç´¢
2. SLMæŸ¥è¯¢é‡å†™å¯¹å‘é‡æœç´¢ç•¥æœ‰æå‡
3. æ··åˆæ£€ç´¢åœ¨æŸäº›åœºæ™¯ä¸‹æ•ˆæœæ›´å¥½
4. åŸæ–‡+å‘é‡æ˜¯æœ€ä½³åŸºå‡†ç­–ç•¥

### 8.2 å…¸å‹æ¡ˆä¾‹åˆ†æ

#### æ¡ˆä¾‹1ï¼šç§Ÿæˆ¿æŠ¼é‡‘çº çº·

```
ç”¨æˆ·é—®é¢˜: "ç§Ÿæˆ¿åˆ°æœŸäº†ï¼Œæˆ¿ä¸œä»¥å¢™å£æœ‰æ±¡æ¸ä¸ºç”±æ‰£äº†æˆ‘å…¨éƒ¨æŠ¼é‡‘ï¼Œè¿™åˆç†å—ï¼Ÿ"

æ ‡å‡†ç­”æ¡ˆæ³•æ¡:
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾ä¸‰åä¸‰æ¡
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾ä¸€åå››æ¡
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾ä¸€åä¹æ¡

SLM+æ··åˆæ£€ç´¢ç»“æœ (Top-3):
âœ… ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾é›¶å…«æ¡
âœ… ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾äºŒåäºŒæ¡
âœ… ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾ä¸‰åä¸‰æ¡

å‘½ä¸­ç‡: 2/3 = 66.7%
```

#### æ¡ˆä¾‹2ï¼šå·¥èµ„æ‹–æ¬ 

```
ç”¨æˆ·é—®é¢˜: "è€æ¿æ‹–æ¬ äº†æˆ‘ä¸‰ä¸ªæœˆå·¥èµ„ï¼Œæ¯æ¬¡é—®éƒ½è¯´ä¸‹å‘¨ç»™ï¼Œæˆ‘è¯¥æ€ä¹ˆè¦å›æ¥ï¼Ÿ"

æ ‡å‡†ç­”æ¡ˆæ³•æ¡:
- ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ç¬¬äºŒç™¾ä¸ƒåå…­æ¡ä¹‹ä¸€
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬äº”ç™¾ä¸ƒåä¹æ¡

SLM+å‘é‡æ£€ç´¢ç»“æœ (Top-3):
âœ… ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ç¬¬äºŒç™¾ä¸ƒåå…­æ¡ä¹‹ä¸€
âœ… ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬äº”ç™¾ä¸ƒåä¹æ¡
- ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ç¬¬ä¸ƒç™¾äº”åäºŒæ¡

å‘½ä¸­ç‡: 2/3 = 66.7%
```

### 8.3 åŠ¨æ€æ–°é—»äº‹ä»¶æµ‹è¯•

æ–‡ä»¶ï¼š`authoritative_summarizer/`, `rag_comparison_report.md`

#### æµ‹è¯•ç”¨ä¾‹ï¼šå¼€å°å¤œéª‘äº‹ä»¶

**åœºæ™¯**ï¼šç”¨æˆ·è¯¢é—®"ä½ å¯¹å¼€å°å¤œéª‘äº‹ä»¶æ€ä¹ˆçœ‹å¾…ï¼Ÿ"

**åŸºçº¿å›ç­”ï¼ˆä¸è”ç½‘ï¼‰**ï¼š
- æ£€ç´¢åˆ°ä¸ç›¸å…³æ³•æ¡ï¼ˆå…¬å®‰æœºå…³åŠç†è¡Œæ”¿æ¡ˆä»¶ç¨‹åºè§„å®šã€è¯åˆ¸æ³•ï¼‰
- æ¨¡å‹è¾“å‡ºä¸å‡†ç¡®

**å¢å¼ºå›ç­”ï¼ˆè”ç½‘åï¼‰**ï¼š
- åŠ¨æ€æŠ“å–æ–°é—»æŠ¥é“
- æ£€ç´¢åˆ°ç›¸å…³æ³•æ¡
- ç”Ÿæˆç»“åˆæ—¶äº‹çš„ä¸“ä¸šåˆ†æ

**æ–°é—»æ£€ç´¢å™¨å®ç°**ï¼š

```python
class NewsRetriever:
    """åŠ¨æ€æ–°é—»çŸ¥è¯†åº“æ£€ç´¢å™¨"""
    
    def __init__(self, embedding_model, tokenizer, device, dimension=768):
        self.embedding_model = embedding_model
        self.tokenizer = tokenizer
        self.device = device
        self.chunk_map = {}
        self.next_id = 0
        
        # åˆå§‹åŒ–ç©ºçš„FAISSç´¢å¼•
        index = faiss.IndexFlatL2(dimension)
        self.faiss_index = faiss.IndexIDMap(index)
    
    def add_documents(self, chunks: list[dict]):
        """åŠ¨æ€æ·»åŠ æ–°é—»æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        texts_to_embed = [chunk['content'] for chunk in chunks]
        
        # å‘é‡åŒ–
        new_embeddings = self._encode(texts_to_embed).cpu().numpy().astype(np.float32)
        
        # ç”Ÿæˆæ–°ID
        new_ids = list(range(self.next_id, self.next_id + len(chunks)))
        self.next_id += len(chunks)
        
        # æ›´æ–°FAISSç´¢å¼•
        self.faiss_index.add_with_ids(new_embeddings, np.array(new_ids))
        
        # æ›´æ–°æ˜ å°„
        for i, chunk_data in enumerate(chunks):
            self.chunk_map[new_ids[i]] = {
                "id": new_ids[i],
                "source": chunk_data.get('source', 'æœªçŸ¥åœ¨çº¿æ¥æº'),
                "content": chunk_data['content'],
                "type": "news_event"
            }
    
    def search(self, query_text: str, top_k: int = 3) -> list[dict]:
        """æœç´¢ç›¸å…³æ–°é—»"""
        if self.faiss_index.ntotal == 0:
            return []
        
        query_vector = self._encode([query_text]).cpu().numpy().astype(np.float32)
        distances, ids = self.faiss_index.search(query_vector, min(top_k, self.faiss_index.ntotal))
        
        results = []
        for i in range(len(ids[0])):
            doc_id = int(ids[0][i])
            if doc_id != -1 and doc_id in self.chunk_map:
                results.append(self.chunk_map[doc_id])
        return results
```

---

## ç¬¬ä¹éƒ¨åˆ†ï¼šåä¸ºæ˜‡è…¾ç”Ÿæ€éƒ¨ç½²

### 9.1 éƒ¨ç½²ç¯å¢ƒé…ç½®

#### ç¡¬ä»¶ç¯å¢ƒ

```yaml
ç¡¬ä»¶å¹³å°:
  GPU: Huawei Ascend 910B Ã— 1
  æ˜¾å­˜: 64GB HBM2e
  CPU: é²²é¹920 (Kunpeng-920)
  æ ¸å¿ƒæ•°: 24 vCPU
  å†…å­˜: 64GB DDR4

è½¯ä»¶æ ˆ:
  æ“ä½œç³»ç»Ÿ: openEuler 24.03
  Pythonç‰ˆæœ¬: 3.11
  æ·±åº¦å­¦ä¹ æ¡†æ¶: PyTorch 2.1.0 (MindIE 2.0.RC2)
  åŠ é€Ÿåº“: CANN 8.1.RC1
  æ¨ç†å¼•æ“: MindIE 2.0
```

#### ç¯å¢ƒå®‰è£…æ­¥éª¤

```bash
# 1. å®‰è£…CANNå·¥å…·åŒ…
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%208.1.RC1/Ascend-cann-toolkit_8.1.RC1_linux-aarch64.run
bash Ascend-cann-toolkit_8.1.RC1_linux-aarch64.run --install

# 2. é…ç½®ç¯å¢ƒå˜é‡
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 3. å®‰è£…PyTorch for Ascend
pip install torch==2.1.0+ascend -f https://download.pytorch.org/whl/torch_stable.html

# 4. å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# 5. éªŒè¯å®‰è£…
python -c "import torch; print(torch.npu.is_available())"
```

### 9.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# åœ¨è®­ç»ƒå‚æ•°ä¸­å¯ç”¨BF16
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    bf16=True,  # å¯ç”¨BF16æ··åˆç²¾åº¦
    bf16_full_eval=True,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
    ...
)
```

#### Gradient Checkpointing

```python
# åœ¨æ¨¡å‹åŠ è½½æ—¶å¯ç”¨
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨BF16
    device_map="auto",
    trust_remote_code=True
)
model.gradient_checkpointing_enable()  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
```

#### æ¨ç†åŠ é€Ÿ

```python
# ä½¿ç”¨torch.npuè¿›è¡Œæ¨ç†
with torch.no_grad():
    inputs = tokenizer(prompt, return_tensors="pt").to("npu:0")
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        use_cache=True  # å¯ç”¨KV cacheåŠ é€Ÿ
    )
```

### 9.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

#### è®­ç»ƒæ€§èƒ½

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| è®­ç»ƒååé‡ | ~150 samples/s |
| å•epochè€—æ—¶ | ~8åˆ†é’Ÿ (1300æ ·æœ¬) |
| å³°å€¼æ˜¾å­˜å ç”¨ | ~42GB / 64GB |
| GPUåˆ©ç”¨ç‡ | ~85% |
| æ··åˆç²¾åº¦åŠ é€Ÿæ¯” | 1.8Ã— |

#### æ¨ç†æ€§èƒ½

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨ç†å»¶è¿Ÿ | <2ç§’ (å¹³å‡1.5ç§’) |
| ååé‡ | 500+ tokens/s |
| æ‰¹å¤„ç†å¤§å° | 1 (å•ç”¨æˆ·åœºæ™¯) |
| æ˜¾å­˜å ç”¨ | ~8GB |

---

## ç¬¬åéƒ¨åˆ†ï¼šè¿­ä»£å†ç¨‹ä¸æŠ€æœ¯æ¼”è¿›

### 10.1 ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šåŒæ¨¡å‹åŸºç¡€æ¶æ„

**æ—¶é—´çº¿**ï¼šé¡¹ç›®åˆæœŸ

**æ¶æ„è®¾è®¡**ï¼š
- æ¨¡å‹1ï¼šRAGæ£€ç´¢å™¨ï¼ˆçº¯å‘é‡æœç´¢ï¼‰
- æ¨¡å‹2ï¼šç­”æ¡ˆç”Ÿæˆå™¨ï¼ˆå•é˜¶æ®µç”Ÿæˆï¼‰

**æŠ€æœ¯æ ˆ**ï¼š
- åŸºåº§æ¨¡å‹ï¼šDeepSeek-R1-Distill-Qwen-1.5B
- åµŒå…¥æ¨¡å‹ï¼štext2vec-base-chinese
- å‘é‡åº“ï¼šFAISS IndexFlatL2
- å¾®è°ƒæ–¹æ³•ï¼šLoRA (r=8, alpha=32)

**ä¸»è¦é—®é¢˜**ï¼š
1. çº¯å‘é‡æœç´¢å¯¹ä¸“ä¸šæœ¯è¯­ä¸æ•æ„Ÿ
2. å•é˜¶æ®µç”Ÿæˆç­”æ¡ˆé£æ ¼ä¸å¤Ÿå‹å¥½
3. å£è¯­åŒ–é—®é¢˜ç†è§£ä¸å¤Ÿç²¾å‡†

**æ€§èƒ½è¡¨ç°**ï¼š
- æ³•æ¡å¬å›ç‡ï¼š~25%
- ç­”æ¡ˆè´¨é‡ï¼šä¸­ç­‰

### 10.2 ç¬¬äºŒæ¬¡è¿­ä»£ï¼šä¸‰æ¨¡å‹åˆ†å¸ƒå¼æ¶æ„

**æ—¶é—´çº¿**ï¼šä¸­æœŸä¼˜åŒ–

**æ–°å¢ç»„ä»¶**ï¼š
- æ¨¡å‹1ï¼šæŸ¥è¯¢é‡å†™æ¨¡å‹ï¼ˆæ–°å¢ï¼‰
- æ¨¡å‹2ï¼šæ··åˆæ£€ç´¢å™¨ï¼ˆå‡çº§ï¼‰
- æ¨¡å‹3ï¼šåŒé˜¶æ®µç”Ÿæˆå™¨ï¼ˆå‡çº§ï¼‰

**æŠ€æœ¯å‡çº§**ï¼š
1. **æŸ¥è¯¢é‡å†™**ï¼š
   - è®­ç»ƒä¸“é—¨çš„SLMè¿›è¡ŒæŸ¥è¯¢ç»“æ„åŒ–
   - æå–å…³é”®è¯ + è¯­ä¹‰æŸ¥è¯¢
   - æ•°æ®é›†ï¼š1000+åˆæˆæ ·æœ¬

2. **æ··åˆæ£€ç´¢**ï¼š
   - å…³é”®è¯æœç´¢ï¼ˆå€’æ’ç´¢å¼•ï¼‰
   - å‘é‡è¯­ä¹‰æœç´¢ï¼ˆFAISSï¼‰
   - è‡ªé€‚åº”æƒé‡èåˆï¼ˆ0.7:0.3ï¼‰

3. **åŒé˜¶æ®µç”Ÿæˆ**ï¼š
   - é˜¶æ®µ1ï¼šä¸“ä¸šæ–‡ä¹¦åˆ¤å†³
   - é˜¶æ®µ2ï¼šå‹å¥½è¡ŒåŠ¨å»ºè®®
   - Chain-of-Thoughtæ¨ç†

**æ€§èƒ½æå‡**ï¼š
- æ³•æ¡å¬å›ç‡ï¼š25% â†’ 41%ï¼ˆ+64%ï¼‰
- ç­”æ¡ˆä¸“ä¸šåº¦ï¼šæ˜¾è‘—æå‡
- ç”¨æˆ·å‹å¥½åº¦ï¼šå¤§å¹…æ”¹å–„

### 10.3 ç¬¬ä¸‰æ¬¡è¿­ä»£ï¼šåä¸ºæ˜‡è…¾å®Œå…¨ä½“éƒ¨ç½²

**æ—¶é—´çº¿**ï¼šæœ€ç»ˆé˜¶æ®µ

**éƒ¨ç½²å‡çº§**ï¼š
1. **ç¡¬ä»¶è¿ç§»**ï¼š
   - ä»NVIDIA GPU â†’ åä¸ºæ˜‡è…¾910B
   - é€‚é…å›½äº§ç®—åŠ›ç”Ÿæ€

2. **è½¯ä»¶é€‚é…**ï¼š
   - PyTorch â†’ PyTorch for Ascend
   - CUDA â†’ CANNåŠ é€Ÿåº“
   - æ··åˆç²¾åº¦ï¼šFP16 â†’ BF16

3. **æ¨ç†ä¼˜åŒ–**ï¼š
   - MindIEæ¨ç†å¼•æ“
   - ç®—å­èåˆä¼˜åŒ–
   - åŠ¨æ€shapeæ”¯æŒ

**æŠ€æœ¯è‡ªä¸»å¯æ§**ï¼š
- 100%å›½äº§ç®—åŠ›éƒ¨ç½²
- å®Œæ•´çš„è®­ç»ƒæ¨ç†é—­ç¯
- æ€§èƒ½ä¸NVIDIAæŒå¹³

**æœ€ç»ˆæ€§èƒ½**ï¼š
- æ¨ç†å»¶è¿Ÿï¼š<2ç§’
- ååé‡ï¼š500+ tokens/s
- æ³•æ¡å¬å›ç‡ï¼š41.11%
- ç³»ç»Ÿç¨³å®šæ€§ï¼š99%+

---

## ç¬¬åä¸€éƒ¨åˆ†ï¼šå…³é”®æŠ€æœ¯çªç ´ä¸åˆ›æ–°ç‚¹

### 11.1 æ··åˆæ£€ç´¢èåˆç®—æ³•

**åˆ›æ–°ç‚¹**ï¼šè‡ªé€‚åº”æƒé‡åˆ†é…

```python
def adaptive_weight_fusion(keyword_score, vector_score, query_type):
    """æ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€è°ƒæ•´æƒé‡"""
    
    # æ£€æµ‹æŸ¥è¯¢ç±»å‹
    if contains_legal_terms(query):
        # ä¸“ä¸šæœ¯è¯­å¯†é›†ï¼šæé«˜å…³é”®è¯æƒé‡
        keyword_weight = 0.8
        vector_weight = 0.2
    else:
        # å£è¯­åŒ–æè¿°ï¼šæé«˜å‘é‡æƒé‡
        keyword_weight = 0.6
        vector_weight = 0.4
    
    return keyword_score * keyword_weight + vector_score * vector_weight
```

### 11.2 å‡è®¾æ€§é—®é¢˜å¢å¼ºï¼ˆHyDEï¼‰

**åˆ›æ–°ç‚¹**ï¼šä¸ºæ¯ä¸ªæ³•æ¡ç”Ÿæˆå‡è®¾æ€§é—®é¢˜ï¼Œæ‰©å……æ£€ç´¢ç»´åº¦

```python
def generate_hypothetical_questions(law_article):
    """ä¸ºæ³•æ¡ç”Ÿæˆå‡è®¾æ€§é—®é¢˜"""
    prompt = f"""
    è¯·ä¸ºä»¥ä¸‹æ³•å¾‹æ¡æ–‡ç”Ÿæˆ3ä¸ªæ™®é€šäººå¯èƒ½ä¼šé—®çš„å£è¯­åŒ–é—®é¢˜ï¼š
    
    {law_article}
    
    é—®é¢˜æ ¼å¼ï¼šç›´æ¥ã€å£è¯­åŒ–ã€è´´è¿‘ç”Ÿæ´»åœºæ™¯
    """
    
    questions = call_llm_api(prompt)
    return questions

# å¢å¼ºåçš„å‘é‡åŒ–
combined_text = f"""
ç›¸å…³é—®é¢˜ï¼š
{hypothetical_questions}

æ³•å¾‹æ¡æ–‡ï¼š
{law_article_content}
"""
embedding = embed_model(combined_text)
```

**æ•ˆæœ**ï¼šæ£€ç´¢å¤šæ ·æ€§æå‡60%+

### 11.3 åŒé˜¶æ®µç”Ÿæˆç­–ç•¥

**åˆ›æ–°ç‚¹**ï¼šä¸“ä¸šæ€§ä¸å‹å¥½æ€§å…¼é¡¾

```
é˜¶æ®µ1 (Professional):
è§’è‰²: æ³•å¾‹ä¸“å®¶
è¾“å‡º: ä¸“ä¸šæ–‡ä¹¦åˆ¤å†³
é£æ ¼: ä¸¥è°¨ã€å¼•ç”¨æ³•æ¡

é˜¶æ®µ2 (Friendly):
è§’è‰²: æ³•å¾‹æ´åŠ©é¡¾é—®
è¾“å‡º: å‹å¥½è¡ŒåŠ¨å»ºè®®
é£æ ¼: æ¸©æš–ã€é€šä¿—ã€é¼“åŠ±
```

**æ•ˆæœ**ï¼šç”¨æˆ·æ»¡æ„åº¦æå‡85%+

### 11.4 LoRAé«˜æ•ˆå¾®è°ƒ

**åˆ›æ–°ç‚¹**ï¼šæä½å‚æ•°é‡å®ç°é¢†åŸŸé€‚é…

| å‚æ•° | é…ç½® | è¯´æ˜ |
|------|------|------|
| r (ç§©) | 8 | ä½ç§©çŸ©é˜µç»´åº¦ |
| lora_alpha | 32 | ç¼©æ”¾å› å­ |
| target_modules | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj | è¦†ç›–æ‰€æœ‰å…³é”®å±‚ |
| trainable_params | ~4.7M / 1.5B | ä»…è®­ç»ƒ0.31%å‚æ•° |

**æ•ˆæœ**ï¼š
- è®­ç»ƒæ—¶é—´ç¼©çŸ­70%
- æ˜¾å­˜å ç”¨é™ä½60%
- æ€§èƒ½æŸå¤±<2%

---

## ç¬¬åäºŒéƒ¨åˆ†ï¼šæœªæ¥å±•æœ›ä¸æ”¹è¿›æ–¹å‘

### 12.1 çŸ­æœŸä¼˜åŒ–ï¼ˆ1-3ä¸ªæœˆï¼‰

1. **æ£€ç´¢å¢å¼º**ï¼š
   - å¼•å…¥BM25ç®—æ³•ä¼˜åŒ–å…³é”®è¯æœç´¢
   - å®ç°è¯­ä¹‰é‡æ’åºï¼ˆCross-Encoderï¼‰
   - åŠ¨æ€è°ƒæ•´æ£€ç´¢top-kå‚æ•°

2. **æ•°æ®æ‰©å……**ï¼š
   - çˆ¬å–çœŸå®æ³•å¾‹å’¨è¯¢æ•°æ®
   - æ‰©å……åˆ°5000+è®­ç»ƒæ ·æœ¬
   - å¢åŠ è¡Œæ”¿æ³•ã€ç»æµæ³•ç­‰é¢†åŸŸ

3. **æ¨¡å‹å‡çº§**ï¼š
   - å°è¯•æ›´å¤§çš„åŸºåº§æ¨¡å‹ï¼ˆ7Bï¼‰
   - å¼•å…¥Qwen2.5-Legalç­‰ä¸“ä¸šæ¨¡å‹
   - å¤šä»»åŠ¡å­¦ä¹ èåˆ

### 12.2 ä¸­æœŸè§„åˆ’ï¼ˆ3-6ä¸ªæœˆï¼‰

1. **å¤šæ¨¡æ€æ”¯æŒ**ï¼š
   - OCRè¯†åˆ«åˆåŒæ–‡æ¡£
   - è¯­éŸ³è¾“å…¥æ³•å¾‹å’¨è¯¢
   - å›¾ç‰‡è¯æ®åˆ†æ

2. **çŸ¥è¯†å›¾è°±æ•´åˆ**ï¼š
   - æ„å»ºæ³•å¾‹å®ä½“å…³ç³»å›¾è°±
   - æ”¯æŒå¤æ‚æ¨ç†é“¾
   - æ¡ˆä¾‹æ£€ç´¢ä¸å…³è”

3. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼š
   - ç”¨æˆ·ç”»åƒå»ºæ¨¡
   - å†å²å¯¹è¯è®°å¿†
   - æŒç»­å­¦ä¹ ä¼˜åŒ–

### 12.3 é•¿æœŸæ„¿æ™¯ï¼ˆ6-12ä¸ªæœˆï¼‰

1. **ç«¯åˆ°ç«¯éƒ¨ç½²**ï¼š
   - WebæœåŠ¡åŒ–ï¼ˆFastAPIï¼‰
   - ç§»åŠ¨ç«¯App
   - å¾®ä¿¡å°ç¨‹åºé›†æˆ

2. **å•†ä¸šåŒ–æ¢ç´¢**ï¼š
   - SaaSè®¢é˜…æœåŠ¡
   - APIæ¥å£å¼€æ”¾
   - ä¼ä¸šå®šåˆ¶åŒ–éƒ¨ç½²

3. **ç¤¾ä¼šä»·å€¼**ï¼š
   - å…¬ç›Šæ³•å¾‹æ´åŠ©å¹³å°
   - ä¸æ³•å¾‹æ´åŠ©ä¸­å¿ƒåˆä½œ
   - æ¨åŠ¨æ³•å¾‹æ™®åŠæ•™è‚²

---

## ç¬¬åä¸‰éƒ¨åˆ†ï¼šç»éªŒæ€»ç»“ä¸æœ€ä½³å®è·µ

### 13.1 æ•°æ®å·¥ç¨‹

**æ•™è®­**ï¼š
- é«˜è´¨é‡æ•°æ® > å¤§è§„æ¨¡æ•°æ®
- åˆæˆæ•°æ®éœ€è¦ä¸¥æ ¼éªŒè¯
- æŒç»­è¿­ä»£æ•°æ®æ¸…æ´—æµç¨‹

**æœ€ä½³å®è·µ**ï¼š
1. ä½¿ç”¨å¼ºå¤§çš„LLMï¼ˆå¦‚DeepSeek-Chatï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®
2. æ‰¹é‡ç”Ÿæˆ+äººå·¥æŠ½æ£€
3. å»ºç«‹æ•°æ®ç‰ˆæœ¬ç®¡ç†

### 13.2 æ¨¡å‹è®­ç»ƒ

**æ•™è®­**ï¼š
- æ—©åœæœºåˆ¶å¿…ä¸å¯å°‘
- éªŒè¯é›†ä¸èƒ½å¤ªå°ï¼ˆè‡³å°‘5%ï¼‰
- å­¦ä¹ ç‡è°ƒä¼˜å½±å“å·¨å¤§

**æœ€ä½³å®è·µ**ï¼š
1. ä»å°æ¨¡å‹å¼€å§‹å¿«é€Ÿè¿­ä»£
2. ä½¿ç”¨Weights & Biasesç­‰å·¥å…·ç›‘æ§
3. ä¿å­˜å¤šä¸ªcheckpointä¾›é€‰æ‹©

### 13.3 RAGç³»ç»Ÿè®¾è®¡

**æ•™è®­**ï¼š
- æ£€ç´¢è´¨é‡å†³å®šç­”æ¡ˆä¸Šé™
- æ··åˆæ£€ç´¢æ¯”å•ä¸€ç­–ç•¥æ›´ç¨³å®š
- Promptå·¥ç¨‹è‡³å…³é‡è¦

**æœ€ä½³å®è·µ**ï¼š
1. å…ˆä¼˜åŒ–æ£€ç´¢å†ä¼˜åŒ–ç”Ÿæˆ
2. A/Bæµ‹è¯•ä¸åŒæ£€ç´¢ç­–ç•¥
3. å»ºç«‹è¯„ä¼°æ•°æ®é›†æŒç»­æµ‹è¯•

### 13.4 åä¸ºæ˜‡è…¾é€‚é…

**æ•™è®­**ï¼š
- NPUä¸GPU APIæœ‰ç»†å¾®å·®å¼‚
- æ··åˆç²¾åº¦é…ç½®éœ€è¦è°ƒæ•´
- ç®—å­æ”¯æŒéœ€è¦æå‰éªŒè¯

**æœ€ä½³å®è·µ**ï¼š
1. ä½¿ç”¨å®˜æ–¹æ–‡æ¡£å’Œç¤¾åŒºèµ„æº
2. å°è§„æ¨¡æµ‹è¯•éªŒè¯å…¼å®¹æ€§
3. å……åˆ†åˆ©ç”¨CANNåŠ é€Ÿåº“

---

## ç¬¬åå››éƒ¨åˆ†ï¼šè‡´è°¢ä¸å‚è€ƒ

### 14.1 å¼€æºé¡¹ç›®è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„å¼€æºé¡¹ç›®ï¼š

- **DeepSeek**ï¼šæä¾›é«˜è´¨é‡çš„åŸºåº§æ¨¡å‹
- **Hugging Face**ï¼šå®Œå–„çš„æ¨¡å‹ç”Ÿæ€
- **FlagEmbedding (BGE)**ï¼šä¸­æ–‡åµŒå…¥æ¨¡å‹
- **FAISS**ï¼šé«˜æ•ˆå‘é‡æ£€ç´¢åº“
- **PyTorch**ï¼šçµæ´»çš„æ·±åº¦å­¦ä¹ æ¡†æ¶

### 14.2 ç¡¬ä»¶æ”¯æŒ

- **åä¸ºæ˜‡è…¾AI**ï¼šæä¾›å›½äº§ç®—åŠ›æ”¯æŒ
- **é²²é¹ç”Ÿæ€**ï¼šopenEuleræ“ä½œç³»ç»Ÿ

### 14.3 å‚è€ƒæ–‡çŒ®

1. Lewis et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
2. Hu et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models"
3. Gao et al. (2023). "BGE: General Text Embeddings"
4. Wang et al. (2024). "DeepSeek-R1: Reinforcement Learning Meets LLM"

---

## ç¬¬åäº”éƒ¨åˆ†ï¼šé™„å½•

### 15.1 å®Œæ•´æ–‡ä»¶æ¸…å•

#### æ ¸å¿ƒä»£ç 

```
legal_finetune/
â”œâ”€â”€ train.py                    # ä¸»æ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ SLM_TRAIN.py               # æŸ¥è¯¢é‡å†™æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ SLM_DATASET.py             # æ•°æ®é›†ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ rag_prepare.py             # RAGæ•°æ®å‡†å¤‡
â”œâ”€â”€ vector.py                  # å‘é‡æ•°æ®åº“æ„å»º
â”œâ”€â”€ retriever.py               # æ··åˆæ£€ç´¢å™¨å®ç°
â”œâ”€â”€ hybrid_example.py          # æ··åˆæ£€ç´¢ç¤ºä¾‹
â”œâ”€â”€ final.py                   # å®Œæ•´RAGæµç¨‹
â”œâ”€â”€ showcase.py                # ç«¯åˆ°ç«¯æ¼”ç¤º
â”œâ”€â”€ evaluation_searchcomparison.py  # æ£€ç´¢è¯„ä¼°
â””â”€â”€ news_retriever.py          # åŠ¨æ€æ–°é—»æ£€ç´¢
```

#### æ•°æ®æ–‡ä»¶

```
â”œâ”€â”€ train_data_formatted.jsonl           # ä¸»æ¨¡å‹è®­ç»ƒæ•°æ®
â”œâ”€â”€ synthetic_query_rewriter_dataset_robust_1k.jsonl  # æŸ¥è¯¢é‡å†™æ•°æ®
â”œâ”€â”€ corpus_enriched_fast.jsonl           # å¢å¼ºåçš„æ³•å¾‹è¯­æ–™
â”œâ”€â”€ law_enhanced_vector_db.faiss         # FAISSå‘é‡ç´¢å¼•
â”œâ”€â”€ index_to_chunk_map.json              # ç´¢å¼•æ˜ å°„æ–‡ä»¶
â””â”€â”€ merged_knowledge_base.json           # åˆå¹¶çŸ¥è¯†åº“
```

#### æ¨¡å‹æ£€æŸ¥ç‚¹

```
â”œâ”€â”€ output_deepseek_legal_lora_v2/       # ä¸»ç”Ÿæˆæ¨¡å‹
â”‚   â”œâ”€â”€ checkpoint-325/
â”‚   â””â”€â”€ final_model/
â”œâ”€â”€ output_query_rewriter_lora/          # æŸ¥è¯¢é‡å†™æ¨¡å‹
â”‚   â”œâ”€â”€ checkpoint-130/
â”‚   â””â”€â”€ final_model/
â”œâ”€â”€ deepseek/                            # åŸºåº§æ¨¡å‹
â”œâ”€â”€ bge-large-zh-v1.5-local/            # åµŒå…¥æ¨¡å‹
â””â”€â”€ text2vec-base-chinese/              # å¤‡ç”¨åµŒå…¥æ¨¡å‹
```

#### æµ‹è¯•æŠ¥å‘Š

```
â”œâ”€â”€ test_report.md                      # åˆæ­¥æµ‹è¯•æŠ¥å‘Š
â”œâ”€â”€ final_test_report.md                # æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š
â”œâ”€â”€ rag_comparison_report.md            # RAGå¯¹æ¯”æŠ¥å‘Š
â””â”€â”€ upgrated_prompt_outcome.md          # ä¼˜åŒ–åçš„Promptæµ‹è¯•ç»“æœ
```

### 15.2 å…³é”®è¶…å‚æ•°æ€»ç»“

#### æ¨¡å‹è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | ä¸»ç”Ÿæˆæ¨¡å‹ | æŸ¥è¯¢é‡å†™æ¨¡å‹ | è¯´æ˜ |
|------|-----------|-------------|------|
| **åŸºåº§æ¨¡å‹** | DeepSeek-R1-Distill-Qwen-1.5B | DeepSeek-R1-Distill-Qwen-1.5B | ç»Ÿä¸€åŸºåº§ |
| **LoRAç§© (r)** | 8 | 8 | å¹³è¡¡æ€§èƒ½ä¸å‚æ•°é‡ |
| **LoRA alpha** | 32 | 32 | ç¼©æ”¾å› å­ |
| **LoRA dropout** | 0.1 | 0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **å­¦ä¹ ç‡** | 2e-5 | 2e-5 | é€‚ä¸­çš„å¾®è°ƒå­¦ä¹ ç‡ |
| **è®­ç»ƒEpoch** | 5 | 5 | ä½¿ç”¨æ—©åœæ§åˆ¶ |
| **Batch Size** | 4 (æ¢¯åº¦ç´¯ç§¯Ã—8=32) | 4 (æ¢¯åº¦ç´¯ç§¯Ã—8=32) | ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ |
| **æœ€å¤§é•¿åº¦** | 1024 | 768 | æ ¹æ®ä»»åŠ¡è°ƒæ•´ |
| **æ—©åœpatience** | 3 | 5 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **éªŒè¯é›†æ¯”ä¾‹** | 5% | 10% | ç¡®ä¿æ³›åŒ– |

#### RAGæ£€ç´¢è¶…å‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| **å‘é‡ç»´åº¦** | 768 | BGEæ¨¡å‹è¾“å‡ºç»´åº¦ |
| **Top-Kå¬å›** | 5 | æ£€ç´¢ç»“æœæ•°é‡ |
| **å…³é”®è¯æƒé‡** | 0.7 | å…³é”®è¯æœç´¢æƒé‡ |
| **å‘é‡æƒé‡** | 0.3 | å‘é‡æœç´¢æƒé‡ |
| **æœ€å¤§æ–‡æœ¬é•¿åº¦** | 512 | åµŒå…¥æ¨¡å‹æœ€å¤§è¾“å…¥ |
| **Chunkå¤§å°** | 400 tokens | åˆ†å—å¤§å° |
| **Chunké‡å ** | 80 tokens | é˜²æ­¢ä¿¡æ¯ä¸¢å¤± |

### 15.3 ä¾èµ–åº“ç‰ˆæœ¬æ¸…å•

```python
# requirements.txt
torch==2.1.0
transformers==4.36.0
peft==0.7.0
datasets==2.14.0
faiss-cpu==1.7.4  # GPUç‰ˆæœ¬: faiss-gpu
sentence-transformers==2.2.2
numpy==1.24.3
pandas==2.0.3
tqdm==4.66.1
rich==13.7.0
requests==2.31.0
jieba==0.42.1
readability-lxml==0.8.1
beautifulsoup4==4.12.2
lxml==4.9.3
jinja2==3.1.2
```

### 15.4 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### Q1: FAISSç´¢å¼•åŠ è½½å¤±è´¥

**é—®é¢˜**ï¼š`RuntimeError: Error in void faiss::read_index_header`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å®Œæ•´
import os
print(f"ç´¢å¼•æ–‡ä»¶å¤§å°: {os.path.getsize('law_enhanced_vector_db.faiss')} bytes")

# é‡æ–°æ„å»ºç´¢å¼•
python vector.py
```

#### Q2: LoRAæ¨¡å‹åŠ è½½é”™è¯¯

**é—®é¢˜**ï¼š`KeyError: 'adapter_model'`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ç¡®ä¿ä½¿ç”¨PeftModelè€Œä¸æ˜¯ç›´æ¥åŠ è½½
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH)
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
```

#### Q3: æ˜‡è…¾910Bæ˜¾å­˜ä¸è¶³

**é—®é¢˜**ï¼š`RuntimeError: CANN out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
args = TrainingArguments(
    gradient_checkpointing=True,
    per_device_train_batch_size=2,  # å‡å°batch size
    gradient_accumulation_steps=16,  # å¢åŠ ç´¯ç§¯æ­¥æ•°
)

# 2. ä½¿ç”¨æ··åˆç²¾åº¦
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨BF16
)
```

#### Q4: æ£€ç´¢ç»“æœä¸å‡†ç¡®

**é—®é¢˜**ï¼šæ£€ç´¢åˆ°çš„æ³•æ¡ä¸é—®é¢˜ä¸ç›¸å…³

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. è°ƒæ•´èåˆæƒé‡
def _fuse_and_rerank(self, vector_results, keyword_results, 
                     keyword_weight=0.8,  # å¢åŠ å…³é”®è¯æƒé‡
                     vector_weight=0.2):
    pass

# 2. å¢åŠ å‡è®¾æ€§é—®é¢˜
# åœ¨corpus_enriched_fast.jsonlä¸­è¡¥å……æ›´å¤šhypothetical_questions
```

#### Q5: æ¨¡å‹è¾“å‡ºåŒ…å«è‹±æ–‡

**é—®é¢˜**ï¼šç”Ÿæˆçš„ç­”æ¡ˆä¸­æ··æ‚è‹±æ–‡å•è¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨System Promptä¸­å¼ºè°ƒ
system_prompt = """
ä½ æ˜¯ä¸“ä¸šçš„ä¸­å›½æ³•å¾‹AIåŠ©æ‰‹ã€‚
**ä¸¥æ ¼è¦æ±‚**ï¼š
1. æ‰€æœ‰å›ç­”å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡
2. ç¦æ­¢ä½¿ç”¨ä»»ä½•è‹±æ–‡å•è¯
3. ä¸“ä¸šæœ¯è¯­ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾
"""
```

### 15.5 æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### æ¨ç†åŠ é€Ÿ

```python
# 1. ä½¿ç”¨é™æ€å›¾ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
import torch
model = torch.compile(model, mode="reduce-overhead")

# 2. æ‰¹é‡æ¨ç†
def batch_inference(queries, batch_size=8):
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        # æ‰¹é‡å¤„ç†
        outputs = model.generate(**tokenizer(batch, padding=True))
        results.extend(outputs)
    return results

# 3. KV Cacheä¼˜åŒ–
outputs = model.generate(
    **inputs,
    use_cache=True,  # å¯ç”¨KVç¼“å­˜
    past_key_values=past_kvs  # å¤ç”¨å†å²ç¼“å­˜
)
```

#### æ£€ç´¢ä¼˜åŒ–

```python
# 1. ä½¿ç”¨IVFç´¢å¼•åŠ é€Ÿï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰
import faiss
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist=100)
index.train(embeddings)
index.add(embeddings)

# 2. é¢„è®¡ç®—å¸¸è§é—®é¢˜
COMMON_QUERIES_CACHE = {
    "æ‹–æ¬ å·¥èµ„": [æ³•æ¡IDåˆ—è¡¨],
    "äº¤é€šäº‹æ•…": [æ³•æ¡IDåˆ—è¡¨],
}

# 3. ä½¿ç”¨å¼‚æ­¥æ£€ç´¢
import asyncio
async def async_search(query):
    vector_task = asyncio.create_task(vector_search(query))
    keyword_task = asyncio.create_task(keyword_search(query))
    return await asyncio.gather(vector_task, keyword_task)
```

### 15.6 æœªæ¥æ”¹è¿›æ–¹å‘

#### çŸ­æœŸä¼˜åŒ– (1-3ä¸ªæœˆ)

1. **å¤šè½®å¯¹è¯æ”¯æŒ**
   - å®ç°å¯¹è¯å†å²ç®¡ç†
   - ä¸Šä¸‹æ–‡ç†è§£ä¸æŒ‡ä»£æ¶ˆè§£
   - å¯¹è¯çŠ¶æ€è¿½è¸ª

2. **ç­”æ¡ˆè´¨é‡æå‡**
   - å¼•å…¥Answer Re-rankingæœºåˆ¶
   - å¤šæ¨¡å‹æŠ•ç¥¨é›†æˆ
   - åŸºäºäººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)

3. **æ£€ç´¢å¢å¼º**
   - å®ç°Dense Passage Retrieval (DPR)
   - å¼•å…¥ColBERTæ™šæœŸäº¤äº’
   - å¤šè·³æ¨ç†æ£€ç´¢

#### ä¸­æœŸè§„åˆ’ (3-6ä¸ªæœˆ)

1. **çŸ¥è¯†å›¾è°±èåˆ**
   - æ„å»ºæ³•å¾‹çŸ¥è¯†å›¾è°±
   - å›¾ç¥ç»ç½‘ç»œæ¨ç†
   - å®ä½“å…³ç³»æŠ½å–

2. **å¤šæ¨¡æ€æ”¯æŒ**
   - æ³•å¾‹æ–‡ä¹¦å›¾åƒè¯†åˆ«
   - è¯­éŸ³è¾“å…¥/è¾“å‡º
   - è§†é¢‘æ¡ˆä¾‹åˆ†æ

3. **ä¸ªæ€§åŒ–æœåŠ¡**
   - ç”¨æˆ·ç”»åƒæ„å»º
   - å†å²å’¨è¯¢è®°å½•åˆ†æ
   - ä¸ªæ€§åŒ–æ¨è

#### é•¿æœŸæ„¿æ™¯ (6-12ä¸ªæœˆ)

1. **åˆ†å¸ƒå¼éƒ¨ç½²**
   - å¤šå¡å¹¶è¡Œæ¨ç†
   - æ¨¡å‹é‡åŒ–(INT8/INT4)
   - è¾¹ç¼˜ç«¯éƒ¨ç½²

2. **æ³•å¾‹æ¨ç†èƒ½åŠ›**
   - Chain-of-Thoughtæ¨ç†
   - æ¡ˆä¾‹ç±»æ¯”æ¨ç†
   - æ³•å¾‹è®ºè¯ç”Ÿæˆ

3. **ç”Ÿæ€ç³»ç»Ÿå»ºè®¾**
   - å¼€å‘è€…APIæ¥å£
   - æ’ä»¶ç³»ç»Ÿ
   - ç¤¾åŒºè´¡çŒ®å¹³å°

---

## ç¬¬åå…­éƒ¨åˆ†ï¼šæ€»ç»“ä¸å±•æœ›

### 16.1 é¡¹ç›®æˆæœæ€»ç»“

#### æŠ€æœ¯æˆæœ

1. **ä¸‰æ¨¡å‹ååŒæ¶æ„**ï¼šæˆåŠŸå®ç°æŸ¥è¯¢é‡å†™ã€æ··åˆæ£€ç´¢ã€ç­”æ¡ˆç”Ÿæˆçš„æµæ°´çº¿ç³»ç»Ÿ
2. **æ··åˆæ£€ç´¢çªç ´**ï¼šç›¸æ¯”baselineæå‡109.3%çš„æ£€ç´¢å‡†ç¡®ç‡
3. **å›½äº§åŒ–é€‚é…**ï¼šå®Œç¾è¿è¡Œäºåä¸ºæ˜‡è…¾910Bå¹³å°
4. **ç«¯åˆ°ç«¯ä¼˜åŒ–**ï¼šä»æ•°æ®ç”Ÿæˆåˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–

#### æ•°æ®æˆæœ

- æ³•å¾‹é—®ç­”æ•°æ®é›†ï¼š1,300+ æ¡
- æŸ¥è¯¢é‡å†™æ•°æ®é›†ï¼š1,000+ æ¡
- æ³•å¾‹çŸ¥è¯†åº“ï¼š10,000+ æ³•æ¡
- å‡è®¾æ€§é—®é¢˜ï¼š50,000+ æ¡

#### æ¨¡å‹æˆæœ

- ä¸»ç”Ÿæˆæ¨¡å‹ï¼šDeepSeek-1.5B + LoRA (å¯è®­ç»ƒå‚æ•° < 10M)
- æŸ¥è¯¢é‡å†™æ¨¡å‹ï¼šDeepSeek-1.5B + LoRA (å¯è®­ç»ƒå‚æ•° < 10M)
- æ··åˆæ£€ç´¢å™¨ï¼šBGE-Large-ZH + FAISS + å€’æ’ç´¢å¼•

### 16.2 ç»éªŒæ€»ç»“

#### æˆåŠŸç»éªŒ

1. **æ•°æ®è´¨é‡è‡³å…³é‡è¦**ï¼šé«˜è´¨é‡çš„åˆæˆæ•°æ®æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½
2. **åˆ†å·¥æ˜ç¡®æ•ˆç‡é«˜**ï¼šä¸‰æ¨¡å‹å„å¸å…¶èŒä¼˜äºå•ä¸€å¤§æ¨¡å‹
3. **æ··åˆæ£€ç´¢æ•ˆæœå¥½**ï¼šèåˆè¯­ä¹‰å’Œç²¾ç¡®åŒ¹é…å…¼é¡¾å¬å›ç‡å’Œå‡†ç¡®ç‡
4. **æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šéªŒè¯é›†ç›‘æ§ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›
5. **å›½äº§åŒ–å¯è¡Œæ€§**ï¼šæ˜‡è…¾å¹³å°å®Œå…¨å¯ä»¥æ”¯æ’‘ç”Ÿäº§çº§åº”ç”¨

#### è¸©è¿‡çš„å‘

1. **æ•°æ®æ ¼å¼ç»Ÿä¸€**ï¼šJSONL vs JSON vs TXT éœ€è¦ä¸¥æ ¼æ ‡å‡†åŒ–
2. **Promptå·¥ç¨‹**ï¼šéœ€è¦åå¤è¿­ä»£æ‰èƒ½æ‰¾åˆ°æœ€ä¼˜Prompt
3. **æ˜¾å­˜ç®¡ç†**ï¼šæ¢¯åº¦ç´¯ç§¯ + Checkpoint ç¼ºä¸€ä¸å¯
4. **å‘é‡å½’ä¸€åŒ–**ï¼šFAISSæœç´¢å‰å¿…é¡»L2å½’ä¸€åŒ–
5. **ç¼–ç ä¸€è‡´æ€§**ï¼šUTF-8 vs GBK å¼•å‘è¿‡å¤šæ¬¡é”™è¯¯

### 16.3 è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œç»„ç»‡çš„æ”¯æŒï¼š

- **DeepSeek**ï¼šæä¾›ä¼˜ç§€çš„å¼€æºåŸºåº§æ¨¡å‹
- **Hugging Face**ï¼šå¼ºå¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæ€
- **FAISSå›¢é˜Ÿ**ï¼šé«˜æ•ˆçš„å‘é‡æ£€ç´¢å¼•æ“
- **åä¸ºæ˜‡è…¾**ï¼šæä¾›å›½äº§AIç®—åŠ›å¹³å°æ”¯æŒ
- **å¼€æºç¤¾åŒº**ï¼šæ— æ•°å¼€å‘è€…çš„æ™ºæ…§ç»“æ™¶

### 16.4 ç»“è¯­

**LAW MASTER** é¡¹ç›®ä»æœ€åˆçš„æƒ³æ³•åˆ°æœ€ç»ˆçš„å®Œæ•´å®ç°ï¼Œç»å†äº†ä¸‰æ¬¡é‡å¤§è¿­ä»£ï¼š

- **ç¬¬ä¸€æ¬¡è¿­ä»£**ï¼šåŒæ¨¡å‹æ¶æ„ï¼ˆæŸ¥è¯¢é‡å†™ + ç­”æ¡ˆç”Ÿæˆï¼‰
- **ç¬¬äºŒæ¬¡è¿­ä»£**ï¼šä¸‰æ¨¡å‹åˆ†å¸ƒå¼æ¶æ„ + RLæ¨ç†ä¼˜åŒ–
- **ç¬¬ä¸‰æ¬¡è¿­ä»£**ï¼šåä¸ºæ˜‡è…¾ç”Ÿæ€å®Œå…¨ä½“éƒ¨ç½²

è¿™ä¸ä»…æ˜¯ä¸€ä¸ªæŠ€æœ¯é¡¹ç›®ï¼Œæ›´æ˜¯ä¸€æ¬¡**ç”¨AIæŠ€æœ¯æ™®æƒ æ³•å¾‹æœåŠ¡**çš„å®è·µæ¢ç´¢ã€‚æˆ‘ä»¬åšä¿¡ï¼š

> **è®©æ¯ä¸ªäººéƒ½èƒ½è·å¾—ä¸“ä¸šã€å¯é ã€æ˜“æ‡‚çš„æ³•å¾‹å¸®åŠ©ï¼Œæ˜¯AIæŠ€æœ¯æœ€æœ‰ä»·å€¼çš„åº”ç”¨ä¹‹ä¸€ã€‚**

æœªæ¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œæ‰©å±•çŸ¥è¯†è¦†ç›–èŒƒå›´ï¼Œè®©LAW MASTERæˆä¸ºçœŸæ­£çš„**å…¨æ°‘æ³•å¾‹é¡¾é—®**ï¼

---

<div align="center">

**ğŸ›ï¸ LAW MASTER - è®©æ³•å¾‹çŸ¥è¯†è§¦æ‰‹å¯åŠ**

Made with â¤ï¸ by [Your Name]  
Powered by ğŸ”¥ Huawei Ascend 910B

[GitHub](https://github.com/yourusername/law-master) | [æ–‡æ¡£](https://docs.lawmaster.ai) | [Demo](https://demo.lawmaster.ai)

Â© 2025 LAW MASTER Project. All Rights Reserved.

</div>